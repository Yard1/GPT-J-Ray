{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "comet_ml is installed but `COMET_API_KEY` is not set.\n",
      "/home/ray/anaconda3/lib/python3.8/site-packages/xgboost/compat.py:31: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "2023-02-09 14:50:46,570\tINFO worker.py:1364 -- Connecting to existing Ray cluster at address: 10.0.48.108:6379...\n",
      "2023-02-09 14:50:46,580\tINFO worker.py:1544 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2023-02-09 14:50:46,583\tINFO packaging.py:330 -- Pushing file package 'gcs://_ray_pkg_1e3dac5337413e0660dd7da00d1f0b6e.zip' (0.13MiB) to Ray cluster...\n",
      "2023-02-09 14:50:46,585\tINFO packaging.py:343 -- Successfully pushed file package 'gcs://_ray_pkg_1e3dac5337413e0660dd7da00d1f0b6e.zip'.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, AutoModelForCausalLM\n",
    "from transformers import GPTJForCausalLM, AutoTokenizer, DataCollatorForLanguageModeling, default_data_collator\n",
    "from datasets import load_dataset\n",
    "import time\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import ray\n",
    "import ray.data\n",
    "from ray.data.preprocessors import BatchMapper, Chain\n",
    "import os\n",
    "#os.environ[\"RAY_ML_DEV\"] = \"1\"\n",
    "\n",
    "\n",
    "\n",
    "ray.init(runtime_env={\"env_vars\": {\"NCCL_SOCKET_IFNAME\": \"ens5\"}})\n",
    "start = time.time()\n",
    "name = \"gpt-j-6B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset tiny_shakespeare (/home/ray/.cache/huggingface/datasets/tiny_shakespeare/default/1.0.0/b5b13969f09fe8707337f6cb296314fbe06960bd9a868dca39e713e163d27b5e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d27d4df15f454daeb4f7666092ac1ae1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Loading dataset\")\n",
    "# current_dataset = load_dataset(\"wikitext\", 'wikitext-103-v1', split=\"train\")\n",
    "current_dataset = load_dataset(\"tiny_shakespeare\")\n",
    "current_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(current_dataset, dict):\n",
    "    ray_datasets = ray.data.from_huggingface(current_dataset)\n",
    "else:\n",
    "    ray_dataset: ray.data.Dataset = ray.data.from_huggingface(current_dataset)\n",
    "    train, validation, test = ray_dataset.random_shuffle(seed=1).split_proportionately([0.9])\n",
    "    ray_datasets = {\"train\": train.repartition(16), \"validation\": validation.repartition(4)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 1024\n",
    "def split_column_with_one_string(df):\n",
    "    data = df[\"text\"].iloc[0]\n",
    "    df = pd.DataFrame()\n",
    "    #df[\"text\"] = [x.strip() for x in data.split(\"\\n\\n\") if x.strip()]\n",
    "    df[\"text\"] = [data[i:i+block_size].strip() for i in range(0, len(data), block_size)]\n",
    "    return df\n",
    "\n",
    "string_splitter = BatchMapper(split_column_with_one_string, batch_format=\"pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.data.preprocessor import Preprocessor\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, pretrained_model_name_or_path, caption_column, revision=None) -> None:\n",
    "        # Importing here to work around a memory leak with Ray Data in 2.2\n",
    "        # Should be fixed in 2.3 or 2.4\n",
    "        from transformers import AutoTokenizer\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path, revision=revision)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.caption_column = caption_column\n",
    "\n",
    "    # Preprocessing the datasets.\n",
    "    # We need to tokenize input captions and transform the images.\n",
    "    def tokenize_captions(self, txt_list, is_train=True):\n",
    "        tokens = self.tokenizer(list(txt_list[self.caption_column]), truncation=True,\n",
    "                                       max_length=self.tokenizer.model_max_length, padding=\"max_length\",return_tensors=\"np\",)        \n",
    "        tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
    "        return {k: v for k, v in tokens.items()}\n",
    "\n",
    "    def __call__(self, df: \"pd.DataFrame\") -> \"pd.DataFrame\":\n",
    "        return self.tokenize_captions(df)\n",
    "\n",
    "\n",
    "class TokenizerPreprocessor(Preprocessor):\n",
    "    _is_fittable = False\n",
    "\n",
    "    def __init__(self, pretrained_model_name_or_path, caption_column, revision=None) -> None:\n",
    "        self.pretrained_model_name_or_path = pretrained_model_name_or_path\n",
    "        self.revision = revision\n",
    "        self.caption_column = caption_column\n",
    "\n",
    "    _transform_pandas = Tokenizer\n",
    "\n",
    "    def _get_transform_config(self):\n",
    "        \"\"\"Returns kwargs to be passed to :meth:`ray.data.Dataset.map_batches`.\n",
    "        This can be implemented by subclassing preprocessors.\n",
    "        \"\"\"\n",
    "        return dict(\n",
    "            compute=ray.data.ActorPoolStrategy(),\n",
    "            fn_constructor_kwargs=dict(\n",
    "                pretrained_model_name_or_path=self.pretrained_model_name_or_path,\n",
    "                revision=self.revision,\n",
    "                caption_column=self.caption_column,\n",
    "            ),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "\n",
    "class SM3(Optimizer):\n",
    "    \"\"\"Implements SM3 algorithm.\n",
    "    It has been proposed in `Memory-Efficient Adaptive Optimization`_.\n",
    "    Arguments:\n",
    "        params (iterable): iterable of parameters to optimize or dicts defining\n",
    "            parameter groups\n",
    "        lr (float, optional): coefficient that scale delta before it is applied\n",
    "            to the parameters (default: 0.1)\n",
    "        momentum (float, optional): coefficient used to scale prior updates\n",
    "            before adding. This drastically increases memory usage if\n",
    "            `momentum > 0.0`. This is ignored if the parameter's gradient\n",
    "            is sparse. (default: 0.0)\n",
    "        beta (float, optional): coefficient used for exponential moving\n",
    "            averages (default: 0.0)\n",
    "        eps (float, optional): Term added to square-root in denominator to\n",
    "            improve numerical stability (default: 1e-30)\n",
    "    .. _Memory-Efficient Adaptive Optimization:\n",
    "        https://arxiv.org/abs/1901.11150\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=0.1, momentum=0.0, beta=0.0, eps=1e-30):\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(\"Invalid learning rate: {0}\".format(lr))\n",
    "        if not 0.0 <= momentum < 1.0:\n",
    "            raise ValueError(\"Invalid momentum: {0}\".format(momentum))\n",
    "        if not 0.0 <= beta < 1.0:\n",
    "            raise ValueError(\"Invalid beta: {0}\".format(beta))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(\"Invalid eps: {0}\".format(eps))\n",
    "\n",
    "        defaults = {\"lr\": lr, \"momentum\": momentum, \"beta\": beta, \"eps\": eps}\n",
    "        super(SM3, self).__init__(params, defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            momentum = group[\"momentum\"]\n",
    "            beta = group[\"beta\"]\n",
    "            eps = group[\"eps\"]\n",
    "            for p in group[\"params\"]:\n",
    "                if p is None:\n",
    "                    continue\n",
    "                grad = p.grad\n",
    "\n",
    "                state = self.state[p]\n",
    "                shape = grad.shape\n",
    "                rank = len(shape)\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state[\"step\"] = 0\n",
    "                    state[\"momentum_buffer\"] = 0.0\n",
    "                    _add_initial_accumulators(state, grad)\n",
    "\n",
    "                if grad.is_sparse:\n",
    "                    # the update is non-linear so indices must be unique\n",
    "                    grad.coalesce()\n",
    "                    grad_indices = grad._indices()\n",
    "                    grad_values = grad._values()\n",
    "\n",
    "                    # Transform update_values into sparse tensor\n",
    "                    def make_sparse(values):\n",
    "                        constructor = grad.new\n",
    "                        if grad_indices.dim() == 0 or values.dim() == 0:\n",
    "                            return constructor().resize_as_(grad)\n",
    "                        return constructor(grad_indices, values, grad.size())\n",
    "\n",
    "                    acc = state[_key(0)]\n",
    "                    update_values = _compute_sparse_update(\n",
    "                        beta, acc, grad_values, grad_indices\n",
    "                    )\n",
    "\n",
    "                    self._update_sparse_accumulator(\n",
    "                        beta, acc, make_sparse(update_values)\n",
    "                    )\n",
    "\n",
    "                    # Add small amount for numerical stability\n",
    "                    update_values.add_(eps).rsqrt_().mul_(grad_values)\n",
    "\n",
    "                    update = make_sparse(update_values)\n",
    "                else:\n",
    "                    # Get previous accumulators mu_{t-1}\n",
    "                    if rank > 1:\n",
    "                        acc_list = [state[_key(i)] for i in range(rank)]\n",
    "                    else:\n",
    "                        acc_list = [state[_key(0)]]\n",
    "\n",
    "                    # Get update from accumulators and gradients\n",
    "                    update = _compute_update(beta, acc_list, grad)\n",
    "\n",
    "                    # Update accumulators.\n",
    "                    self._update_accumulator(beta, acc_list, update)\n",
    "\n",
    "                    # Add small amount for numerical stability\n",
    "                    update.add_(eps).rsqrt_().mul_(grad)\n",
    "\n",
    "                    if momentum > 0.0:\n",
    "                        m = state[\"momentum_buffer\"]\n",
    "                        update.mul_(1.0 - momentum).add_(m, alpha=momentum)\n",
    "                        state[\"momentum_buffer\"] = update.detach()\n",
    "\n",
    "                p.sub_(update, alpha=group[\"lr\"])\n",
    "                state[\"step\"] += 1\n",
    "        return loss\n",
    "\n",
    "    @staticmethod\n",
    "    def _update_accumulator(beta, acc_list, update):\n",
    "        for i, acc in enumerate(acc_list):\n",
    "            nu_max = _max_reduce_except_dim(update, i)\n",
    "            if beta > 0.0:\n",
    "                torch.max(acc, nu_max, out=acc)\n",
    "            else:\n",
    "                # No need to compare - nu_max is bigger because of grad ** 2\n",
    "                acc.copy_(nu_max)\n",
    "\n",
    "    @staticmethod\n",
    "    def _update_sparse_accumulator(beta, acc, update):\n",
    "        nu_max = _max_reduce_except_dim(update.to_dense(), 0).squeeze()\n",
    "        if beta > 0.0:\n",
    "            torch.max(acc, nu_max, out=acc)\n",
    "        else:\n",
    "            # No need to compare - nu_max is bigger because of grad ** 2\n",
    "            acc.copy_(nu_max)\n",
    "\n",
    "\n",
    "def _compute_sparse_update(beta, acc, grad_values, grad_indices):\n",
    "    # In the sparse case, a single accumulator is used.\n",
    "    update_values = torch.gather(acc, 0, grad_indices[0])\n",
    "    if beta > 0.0:\n",
    "        update_values.mul_(beta)\n",
    "    update_values.addcmul_(grad_values, grad_values, value=1.0 - beta)\n",
    "    return update_values\n",
    "\n",
    "\n",
    "def _compute_update(beta, acc_list, grad):\n",
    "    rank = len(acc_list)\n",
    "    update = acc_list[0].clone()\n",
    "    for i in range(1, rank):\n",
    "        # We rely on broadcasting to get the proper end shape.\n",
    "        update = torch.min(update, acc_list[i])\n",
    "    if beta > 0.0:\n",
    "        update.mul_(beta)\n",
    "    update.addcmul_(grad, grad, value=1.0 - beta)\n",
    "\n",
    "    return update\n",
    "\n",
    "\n",
    "def _key(i):\n",
    "    # Returns key used for accessing accumulators\n",
    "    return \"accumulator_\" + str(i)\n",
    "\n",
    "\n",
    "def _add_initial_accumulators(state, grad):\n",
    "    # Creates initial accumulators. For a dense tensor of shape (n1, n2, n3),\n",
    "    # then our initial accumulators are of shape (n1, 1, 1), (1, n2, 1) and\n",
    "    # (1, 1, n3). For a sparse tensor of shape (n, *), we use a single\n",
    "    # accumulator of shape (n,).\n",
    "    shape = grad.shape\n",
    "    rank = len(shape)\n",
    "    defaults = {\"device\": grad.device, \"dtype\": grad.dtype}\n",
    "    acc = {}\n",
    "\n",
    "    if grad.is_sparse:\n",
    "        acc[_key(0)] = torch.zeros(shape[0], **defaults)\n",
    "    elif rank == 0:\n",
    "        # The scalar case is handled separately\n",
    "        acc[_key(0)] = torch.zeros(shape, **defaults)\n",
    "    else:\n",
    "        for i in range(rank):\n",
    "            acc_shape = [1] * i + [shape[i]] + [1] * (rank - 1 - i)\n",
    "            acc[_key(i)] = torch.zeros(acc_shape, **defaults)\n",
    "\n",
    "    state.update(acc)\n",
    "\n",
    "\n",
    "def _max_reduce_except_dim(tensor, dim):\n",
    "    # Computes max along all dimensions except the given dim.\n",
    "    # If tensor is a scalar, it returns tensor.\n",
    "    rank = len(tensor.shape)\n",
    "    result = tensor\n",
    "    if rank > 0:\n",
    "        assert dim < rank\n",
    "        for d in range(rank):\n",
    "            if d != dim:\n",
    "                result = result.max(dim=d, keepdim=True).values\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.utils.logging import disable_progress_bar, enable_progress_bar\n",
    "from transformers.utils.hub import cached_file\n",
    "from accelerate.big_modeling import get_balanced_memory, infer_auto_device_map, load_checkpoint_and_dispatch, init_empty_weights\n",
    "from deepspeed.ops.adam.cpu_adam import DeepSpeedCPUAdam\n",
    "from ray.air import session\n",
    "import torch\n",
    "import os\n",
    "from transformers.trainer_pt_utils import get_parameter_names\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "num_cpus = 8\n",
    "\n",
    "class TrainingArgumentsPatched(TrainingArguments):\n",
    "    @property\n",
    "    def place_model_on_device(self):\n",
    "        return False\n",
    "\n",
    "def trainer_init_per_worker(train_dataset, eval_dataset = None, **config):\n",
    "    # Env vars necessary for HF to setup DDP\n",
    "    #os.environ.pop(\"RANK\")\n",
    "    #os.environ.pop(\"WORLD_SIZE\")\n",
    "    #os.environ.pop(\"LOCAL_RANK\")\n",
    "\n",
    "    os.environ[\"OMP_NUM_THREADS\"] = str(num_cpus)\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "    batch_size = 2\n",
    "    deepspeed = {\n",
    "        \"fp16\": {\n",
    "            \"enabled\": \"auto\",\n",
    "            \"initial_scale_power\": 32,\n",
    "        },\n",
    "        \"bf16\":{\n",
    "            \"enabled\":\"auto\"\n",
    "        },    \n",
    "        \"optimizer\": {\n",
    "            \"type\": \"AdamW\",\n",
    "            \"params\": {\n",
    "            \"lr\": \"auto\",\n",
    "            \"betas\": \"auto\",\n",
    "            \"eps\":\"auto\",\n",
    "            }\n",
    "        },\n",
    "        \"zero_optimization\": {\n",
    "            \"stage\": 3,\n",
    "            \"offload_optimizer\": {\n",
    "                \"device\": \"cpu\",\n",
    "                \"pin_memory\": False,\n",
    "            },\n",
    "           # \"offload_param\": {\n",
    "           #     \"device\": \"cpu\",\n",
    "           #     \"pin_memory\": False,\n",
    "           # },\n",
    "            \"overlap_comm\": True,\n",
    "            \"contiguous_gradients\": True,\n",
    "            \"reduce_bucket_size\": \"auto\",\n",
    "            \"stage3_prefetch_bucket_size\": \"auto\",\n",
    "            \"stage3_param_persistence_threshold\": \"auto\",\n",
    "            \"gather_16bit_weights_on_model_save\": True,\n",
    "            \"round_robin_gradients\": True,\n",
    "        },\n",
    "        \"gradient_accumulation_steps\": \"auto\",\n",
    "        \"gradient_clipping\": \"auto\",\n",
    "        \"steps_per_print\": 1,\n",
    "        \"train_batch_size\": \"auto\",\n",
    "        \"train_micro_batch_size_per_gpu\": \"auto\",\n",
    "        \"wall_clock_breakdown\": False\n",
    "    }\n",
    "\n",
    "    print(\"Preparing training arguments\")\n",
    "    training_args = TrainingArgumentsPatched(\n",
    "        name,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        logging_steps=1, save_strategy=\"steps\",\n",
    "        save_steps=490,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        learning_rate=0.001,\n",
    "        weight_decay=0.01,\n",
    "        # warmup_steps=20,\n",
    "        label_names=['input_ids', 'attention_mask'],  # 'logits', 'past_key_values'\n",
    "        num_train_epochs=config.get(\"epochs\", 2),\n",
    "        push_to_hub=False,\n",
    "        disable_tqdm=True,  # declutter the output a little\n",
    "        bf16=True,\n",
    "        gradient_checkpointing=True,\n",
    "        #local_rank=-1,\n",
    "        # deepspeed=deepspeed\n",
    "    )\n",
    "    # hack\n",
    "    training_args._n_gpu = 0\n",
    "\n",
    "    disable_progress_bar()\n",
    "    dtype = torch.float32\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    print(\"Loading model\")\n",
    "\n",
    "    #with init_empty_weights():\n",
    "    model = GPTJForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\", use_cache=False)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model.parallelize()\n",
    "    # hack\n",
    "    model.is_parallelizable = False\n",
    "\n",
    "    decay_parameters = get_parameter_names(model, [nn.LayerNorm])\n",
    "    decay_parameters = [name for name in decay_parameters if \"bias\" not in name]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if n in decay_parameters],\n",
    "            \"weight_decay\": training_args.weight_decay,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if n not in decay_parameters],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    optimizer = SM3(\n",
    "        optimizer_grouped_parameters,\n",
    "        beta=0,\n",
    "        eps=training_args.adam_epsilon,\n",
    "        lr=training_args.learning_rate,\n",
    "    )\n",
    "\n",
    "    # # Load the checkpoint and dispatch it to the right devices\n",
    "    # model = load_checkpoint_and_dispatch(\n",
    "    #     model,\n",
    "    #     cached_file(\"EleutherAI/gpt-j-6B\", \"pytorch_model.bin\", revision=\"float16\", local_files_only=True),\n",
    "    #     device_map=\"auto\",\n",
    "    #     no_split_module_classes=[\"GPTJBlock\"],\n",
    "    #     dtype=dtype,\n",
    "    # )\n",
    "\n",
    "    enable_progress_bar()\n",
    "\n",
    "    metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        predictions = np.argmax(logits, axis=-1)\n",
    "        return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=default_data_collator,\n",
    "        optimizers=(optimizer, None)\n",
    "    )\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train.huggingface import HuggingFaceTrainer\n",
    "from ray.air.config import RunConfig, ScalingConfig, CheckpointConfig\n",
    "from ray.air.integrations.mlflow import MLflowLoggerCallback\n",
    "from ray.tune import SyncConfig\n",
    "\n",
    "class HuggingFaceTrainerPatched(HuggingFaceTrainer):\n",
    "    def _validate_attributes(self):\n",
    "        for key, conf in self._dataset_config.items():\n",
    "            if conf.use_stream_api:\n",
    "                raise ValueError(\n",
    "                    \"HuggingFaceTrainer does not support `use_stream_api`.\"\n",
    "                )\n",
    "        super(HuggingFaceTrainer, self)._validate_attributes()\n",
    "\n",
    "trainer = HuggingFaceTrainerPatched(\n",
    "    trainer_init_per_worker=trainer_init_per_worker,\n",
    "    scaling_config=ScalingConfig(num_workers=2, use_gpu=True, resources_per_worker={\"GPU\": 8, \"CPU\": 96}),\n",
    "    datasets={\"train\": ray_datasets[\"train\"], \"evaluation\": ray_datasets[\"validation\"]},\n",
    "    run_config=RunConfig(\n",
    "        local_dir=\"/mnt/cluster_storage/\",\n",
    "        sync_config=SyncConfig(syncer=None),\n",
    "        callbacks=[MLflowLoggerCallback(experiment_name=name)],\n",
    "        checkpoint_config=CheckpointConfig(num_to_keep=1, checkpoint_score_attribute=\"eval_loss\", checkpoint_score_order=\"min\"),\n",
    "    ),\n",
    "    preprocessor=Chain(string_splitter, TokenizerPreprocessor(\"EleutherAI/gpt-j-6B\", \"text\")),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-02-09 16:10:56</td></tr>\n",
       "<tr><td>Running for: </td><td>00:07:25.35        </td></tr>\n",
       "<tr><td>Memory:      </td><td>6.6/62.0 GiB       </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Resources requested: 193.0/400 CPUs, 16.0/16 GPUs, 0.0/1159.97 GiB heap, 0.0/390.94 GiB objects\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                           </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  loss</th><th style=\"text-align: right;\">  learning_rate</th><th style=\"text-align: right;\">   epoch</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>HuggingFaceTrainerPatched_5a48d_00000</td><td>RUNNING </td><td>10.0.55.91:18982</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">         423.506</td><td style=\"text-align: right;\">0.9521</td><td style=\"text-align: right;\">    0.000938776</td><td style=\"text-align: right;\">0.122449</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-09 16:03:30,959\tWARNING trial_runner.py:369 -- The maximum number of pending trials has been automatically set to the number of available cluster CPUs, which is high (440 CPUs/pending trials). If you're running an experiment with a large number of trials, this could lead to scheduling overhead. In this case, consider setting the `TUNE_MAX_PENDING_TRIALS_PG` environment variable to the desired maximum number of concurrent trials.\n",
      "(pid=18982, ip=10.0.55.91) /home/ray/anaconda3/lib/python3.8/site-packages/xgboost/compat.py:31: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "(pid=18982, ip=10.0.55.91)   from pandas import MultiIndex, Int64Index\n",
      "(pid=18982, ip=10.0.55.91) comet_ml is installed but `COMET_API_KEY` is not set.\n",
      "(HuggingFaceTrainerPatched pid=18982, ip=10.0.55.91) 2023-02-09 16:03:42,759\tINFO bulk_executor.py:39 -- Executing DAG InputDataBuffer[Input] -> AllToAllOperator[randomize_block_order]\n",
      "(HuggingFaceTrainerPatched pid=18982, ip=10.0.55.91) 2023-02-09 16:03:42,765\tINFO bulk_executor.py:39 -- Executing DAG InputDataBuffer[Input] -> AllToAllOperator[randomize_block_order]\n",
      "(HuggingFaceTrainerPatched pid=18982, ip=10.0.55.91) 2023-02-09 16:03:42,772\tINFO bulk_executor.py:39 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[BatchMapper]\n",
      "(HuggingFaceTrainerPatched pid=18982, ip=10.0.55.91) 2023-02-09 16:03:42,811\tINFO bulk_executor.py:39 -- Executing DAG InputDataBuffer[Input] -> ActorPoolMapOperator[MapBatches(Tokenizer)]\n",
      "(HuggingFaceTrainerPatched pid=18982, ip=10.0.55.91) 2023-02-09 16:03:48,883\tINFO bulk_executor.py:39 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[BatchMapper]\n",
      "(HuggingFaceTrainerPatched pid=18982, ip=10.0.55.91) 2023-02-09 16:03:48,910\tINFO bulk_executor.py:39 -- Executing DAG InputDataBuffer[Input] -> ActorPoolMapOperator[MapBatches(Tokenizer)]\n",
      "(RayTrainWorker pid=13353, ip=10.0.15.54) 2023-02-09 16:03:56,656\tINFO config.py:86 -- Setting up process group for: env:// [rank=0, world_size=2]\n",
      "(HuggingFaceTrainerPatched pid=18982, ip=10.0.55.91) /home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/bulk_dataset_iterator.py:108: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "(HuggingFaceTrainerPatched pid=18982, ip=10.0.55.91)   warnings.warn(\n",
      "(RayTrainWorker pid=13353, ip=10.0.15.54) /home/ray/anaconda3/lib/python3.8/site-packages/xgboost/compat.py:31: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "(RayTrainWorker pid=13353, ip=10.0.15.54)   from pandas import MultiIndex, Int64Index\n",
      "(RayTrainWorker pid=19921, ip=10.0.55.91) /home/ray/anaconda3/lib/python3.8/site-packages/xgboost/compat.py:31: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "(RayTrainWorker pid=19921, ip=10.0.55.91)   from pandas import MultiIndex, Int64Index\n",
      "(RayTrainWorker pid=19921, ip=10.0.55.91) comet_ml is installed but `COMET_API_KEY` is not set.\n",
      "(RayTrainWorker pid=13353, ip=10.0.15.54) comet_ml is installed but `COMET_API_KEY` is not set.\n",
      "(RayTrainWorker pid=19921, ip=10.0.55.91) /home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/bulk_dataset_iterator.py:108: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "(RayTrainWorker pid=19921, ip=10.0.55.91)   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=19921, ip=10.0.55.91) Preparing training arguments\n",
      "(RayTrainWorker pid=13353, ip=10.0.15.54) Preparing training arguments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=13353, ip=10.0.15.54) /home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/bulk_dataset_iterator.py:108: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "(RayTrainWorker pid=13353, ip=10.0.15.54)   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=13353, ip=10.0.15.54) Loading model\n",
      "(RayTrainWorker pid=19921, ip=10.0.55.91) Loading model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=19921, ip=10.0.55.91) Using cuda_amp half precision backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=19921, ip=10.0.55.91) is_model_parallel False\n",
      "(RayTrainWorker pid=19921, ip=10.0.55.91) _n_gpu 0\n",
      "(RayTrainWorker pid=13353, ip=10.0.15.54) is_model_parallel False\n",
      "(RayTrainWorker pid=13353, ip=10.0.15.54) _n_gpu 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=13353, ip=10.0.15.54) Using cuda_amp half precision backend\n",
      "(RayTrainWorker pid=19921, ip=10.0.55.91) ***** Running training *****\n",
      "(RayTrainWorker pid=19921, ip=10.0.55.91)   Num examples = 490\n",
      "(RayTrainWorker pid=19921, ip=10.0.55.91)   Num Epochs = 2\n",
      "(RayTrainWorker pid=19921, ip=10.0.55.91)   Instantaneous batch size per device = 2\n",
      "(RayTrainWorker pid=19921, ip=10.0.55.91)   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "(RayTrainWorker pid=19921, ip=10.0.55.91)   Gradient Accumulation steps = 1\n",
      "(RayTrainWorker pid=19921, ip=10.0.55.91)   Total optimization steps = 490\n",
      "(RayTrainWorker pid=19921, ip=10.0.55.91)   Number of trainable parameters = 6050882784\n",
      "(RayTrainWorker pid=13353, ip=10.0.15.54) ***** Running training *****\n",
      "(RayTrainWorker pid=13353, ip=10.0.15.54)   Num examples = 490\n",
      "(RayTrainWorker pid=13353, ip=10.0.15.54)   Num Epochs = 2\n",
      "(RayTrainWorker pid=13353, ip=10.0.15.54)   Instantaneous batch size per device = 2\n",
      "(RayTrainWorker pid=13353, ip=10.0.15.54)   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "(RayTrainWorker pid=13353, ip=10.0.15.54)   Gradient Accumulation steps = 1\n",
      "(RayTrainWorker pid=13353, ip=10.0.15.54)   Total optimization steps = 490\n",
      "(RayTrainWorker pid=13353, ip=10.0.15.54)   Number of trainable parameters = 6050882784\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                           </th><th style=\"text-align: right;\">  _time_this_iter_s</th><th style=\"text-align: right;\">  _timestamp</th><th style=\"text-align: right;\">  _training_iteration</th><th>date               </th><th>done  </th><th>episodes_total  </th><th style=\"text-align: right;\">   epoch</th><th>experiment_id                   </th><th>hostname     </th><th style=\"text-align: right;\">  iterations_since_restore</th><th style=\"text-align: right;\">  learning_rate</th><th style=\"text-align: right;\">  loss</th><th>node_ip   </th><th style=\"text-align: right;\">  pid</th><th style=\"text-align: right;\">  step</th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  timesteps_since_restore</th><th>timesteps_total  </th><th style=\"text-align: right;\">  training_iteration</th><th>trial_id   </th><th style=\"text-align: right;\">  warmup_time</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>HuggingFaceTrainerPatched_5a48d_00000</td><td style=\"text-align: right;\">            10.9737</td><td style=\"text-align: right;\">  1675987857</td><td style=\"text-align: right;\">                   31</td><td>2023-02-09_16-10-57</td><td>False </td><td>                </td><td style=\"text-align: right;\">0.126531</td><td>dde5cc3d38114e7caf8366ae8cb150f3</td><td>ip-10-0-55-91</td><td style=\"text-align: right;\">                        31</td><td style=\"text-align: right;\">    0.000936735</td><td style=\"text-align: right;\">0.9089</td><td>10.0.55.91</td><td style=\"text-align: right;\">18982</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">             434.479</td><td style=\"text-align: right;\">            10.974</td><td style=\"text-align: right;\">       434.479</td><td style=\"text-align: right;\"> 1675987857</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  31</td><td>5a48d_00000</td><td style=\"text-align: right;\">     0.190475</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=19921, ip=10.0.55.91) [W logger.cpp:317] Warning: Cuda time stats are not collected for multi-device modules. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=19921, ip=10.0.55.91) {'loss': 10.252, 'learning_rate': 0.0009979591836734693, 'epoch': 0.0}\n",
      "(RayTrainWorker pid=13353, ip=10.0.15.54) {'loss': 10.252, 'learning_rate': 0.0009979591836734693, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=13353, ip=10.0.15.54) [W logger.cpp:317] Warning: Cuda time stats are not collected for multi-device modules. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=13353, ip=10.0.15.54) {'loss': 1.9613, 'learning_rate': 0.0009959183673469388, 'epoch': 0.01}\n",
      "(RayTrainWorker pid=19921, ip=10.0.55.91) {'loss': 1.9613, 'learning_rate': 0.0009959183673469388, 'epoch': 0.01}\n",
      "(RayTrainWorker pid=19921, ip=10.0.55.91) {'loss': 5.6013, 'learning_rate': 0.0009938775510204081, 'epoch': 0.01}\n",
      "(RayTrainWorker pid=13353, ip=10.0.15.54) {'loss': 5.6013, 'learning_rate': 0.0009938775510204081, 'epoch': 0.01}\n",
      "(RayTrainWorker pid=13353, ip=10.0.15.54) {'loss': 1.5495, 'learning_rate': 0.0009918367346938776, 'epoch': 0.02}\n",
      "(RayTrainWorker pid=19921, ip=10.0.55.91) {'loss': 1.5495, 'learning_rate': 0.0009918367346938776, 'epoch': 0.02}\n",
      "(RayTrainWorker pid=19921, ip=10.0.55.91) {'loss': 3.9287, 'learning_rate': 0.000989795918367347, 'epoch': 0.02}\n",
      "(RayTrainWorker pid=13353, ip=10.0.15.54) {'loss': 3.9287, 'learning_rate': 0.000989795918367347, 'epoch': 0.02}\n",
      "(RayTrainWorker pid=19921, ip=10.0.55.91) {'loss': 1.4268, 'learning_rate': 0.0009877551020408162, 'epoch': 0.02}\n",
      "(RayTrainWorker pid=13353, ip=10.0.15.54) {'loss': 1.4268, 'learning_rate': 0.0009877551020408162, 'epoch': 0.02}\n",
      "(RayTrainWorker pid=19921, ip=10.0.55.91) {'loss': 1.3279, 'learning_rate': 0.0009857142857142857, 'epoch': 0.03}\n",
      "(RayTrainWorker pid=13353, ip=10.0.15.54) {'loss': 1.3279, 'learning_rate': 0.0009857142857142857, 'epoch': 0.03}\n",
      "(RayTrainWorker pid=13353, ip=10.0.15.54) {'loss': 1.9214, 'learning_rate': 0.0009836734693877552, 'epoch': 0.03}\n",
      "(RayTrainWorker pid=19921, ip=10.0.55.91) {'loss': 1.9214, 'learning_rate': 0.0009836734693877552, 'epoch': 0.03}\n",
      "(RayTrainWorker pid=13353, ip=10.0.15.54) {'loss': 1.2252, 'learning_rate': 0.0009816326530612245, 'epoch': 0.04}\n",
      "(RayTrainWorker pid=19921, ip=10.0.55.91) {'loss': 1.2252, 'learning_rate': 0.0009816326530612245, 'epoch': 0.04}\n",
      "(RayTrainWorker pid=19921, ip=10.0.55.91) {'loss': 1.138, 'learning_rate': 0.0009795918367346938, 'epoch': 0.04}\n",
      "(RayTrainWorker pid=13353, ip=10.0.15.54) {'loss': 1.138, 'learning_rate': 0.0009795918367346938, 'epoch': 0.04}\n",
      "(RayTrainWorker pid=13353, ip=10.0.15.54) {'loss': 1.086, 'learning_rate': 0.0009775510204081633, 'epoch': 0.04}\n",
      "(RayTrainWorker pid=19921, ip=10.0.55.91) {'loss': 1.086, 'learning_rate': 0.0009775510204081633, 'epoch': 0.04}\n",
      "(RayTrainWorker pid=13353, ip=10.0.15.54) {'loss': 1.0577, 'learning_rate': 0.0009755102040816327, 'epoch': 0.05}\n",
      "(RayTrainWorker pid=19921, ip=10.0.55.91) {'loss': 1.0577, 'learning_rate': 0.0009755102040816327, 'epoch': 0.05}\n",
      "(RayTrainWorker pid=13353, ip=10.0.15.54) {'loss': 1.0932, 'learning_rate': 0.000973469387755102, 'epoch': 0.05}\n",
      "(RayTrainWorker pid=19921, ip=10.0.55.91) {'loss': 1.0932, 'learning_rate': 0.000973469387755102, 'epoch': 0.05}\n",
      "(RayTrainWorker pid=19921, ip=10.0.55.91) {'loss': 1.0314, 'learning_rate': 0.0009714285714285714, 'epoch': 0.06}\n",
      "(RayTrainWorker pid=13353, ip=10.0.15.54) {'loss': 1.0314, 'learning_rate': 0.0009714285714285714, 'epoch': 0.06}\n",
      "(RayTrainWorker pid=13353, ip=10.0.15.54) {'loss': 1.1101, 'learning_rate': 0.0009693877551020408, 'epoch': 0.06}\n",
      "(RayTrainWorker pid=19921, ip=10.0.55.91) {'loss': 1.1101, 'learning_rate': 0.0009693877551020408, 'epoch': 0.06}\n",
      "(RayTrainWorker pid=19921, ip=10.0.55.91) {'loss': 1.0382, 'learning_rate': 0.0009673469387755102, 'epoch': 0.07}\n",
      "(RayTrainWorker pid=13353, ip=10.0.15.54) {'loss': 1.0382, 'learning_rate': 0.0009673469387755102, 'epoch': 0.07}\n",
      "(RayTrainWorker pid=13353, ip=10.0.15.54) {'loss': 1.064, 'learning_rate': 0.0009653061224489796, 'epoch': 0.07}\n",
      "(RayTrainWorker pid=19921, ip=10.0.55.91) {'loss': 1.064, 'learning_rate': 0.0009653061224489796, 'epoch': 0.07}\n",
      "(RayTrainWorker pid=13353, ip=10.0.15.54) {'loss': 1.0265, 'learning_rate': 0.000963265306122449, 'epoch': 0.07}\n",
      "(RayTrainWorker pid=19921, ip=10.0.55.91) {'loss': 1.0265, 'learning_rate': 0.000963265306122449, 'epoch': 0.07}\n",
      "(RayTrainWorker pid=13353, ip=10.0.15.54) {'loss': 1.0073, 'learning_rate': 0.0009612244897959183, 'epoch': 0.08}\n",
      "(RayTrainWorker pid=19921, ip=10.0.55.91) {'loss': 1.0073, 'learning_rate': 0.0009612244897959183, 'epoch': 0.08}\n",
      "(RayTrainWorker pid=13353, ip=10.0.15.54) {'loss': 1.0223, 'learning_rate': 0.0009591836734693877, 'epoch': 0.08}\n",
      "(RayTrainWorker pid=19921, ip=10.0.55.91) {'loss': 1.0223, 'learning_rate': 0.0009591836734693877, 'epoch': 0.08}\n",
      "(RayTrainWorker pid=19921, ip=10.0.55.91) {'loss': 1.0235, 'learning_rate': 0.0009571428571428573, 'epoch': 0.09}\n",
      "(RayTrainWorker pid=13353, ip=10.0.15.54) {'loss': 1.0235, 'learning_rate': 0.0009571428571428573, 'epoch': 0.09}\n",
      "(RayTrainWorker pid=19921, ip=10.0.55.91) {'loss': 1.0364, 'learning_rate': 0.0009551020408163265, 'epoch': 0.09}\n",
      "(RayTrainWorker pid=13353, ip=10.0.15.54) {'loss': 1.0364, 'learning_rate': 0.0009551020408163265, 'epoch': 0.09}\n",
      "(RayTrainWorker pid=19921, ip=10.0.55.91) {'loss': 0.9794, 'learning_rate': 0.000953061224489796, 'epoch': 0.09}\n",
      "(RayTrainWorker pid=13353, ip=10.0.15.54) {'loss': 0.9794, 'learning_rate': 0.000953061224489796, 'epoch': 0.09}\n",
      "(RayTrainWorker pid=19921, ip=10.0.55.91) {'loss': 0.9982, 'learning_rate': 0.0009510204081632652, 'epoch': 0.1}\n",
      "(RayTrainWorker pid=13353, ip=10.0.15.54) {'loss': 0.9982, 'learning_rate': 0.0009510204081632652, 'epoch': 0.1}\n",
      "(RayTrainWorker pid=19921, ip=10.0.55.91) {'loss': 1.0788, 'learning_rate': 0.0009489795918367348, 'epoch': 0.1}\n",
      "(RayTrainWorker pid=13353, ip=10.0.15.54) {'loss': 1.0788, 'learning_rate': 0.0009489795918367348, 'epoch': 0.1}\n",
      "(RayTrainWorker pid=13353, ip=10.0.15.54) {'loss': 0.9839, 'learning_rate': 0.0009469387755102042, 'epoch': 0.11}\n",
      "(RayTrainWorker pid=19921, ip=10.0.55.91) {'loss': 0.9839, 'learning_rate': 0.0009469387755102042, 'epoch': 0.11}\n",
      "(RayTrainWorker pid=13353, ip=10.0.15.54) {'loss': 1.0051, 'learning_rate': 0.0009448979591836734, 'epoch': 0.11}\n",
      "(RayTrainWorker pid=19921, ip=10.0.55.91) {'loss': 1.0051, 'learning_rate': 0.0009448979591836734, 'epoch': 0.11}\n",
      "(RayTrainWorker pid=13353, ip=10.0.15.54) {'loss': 0.9748, 'learning_rate': 0.0009428571428571429, 'epoch': 0.11}\n",
      "(RayTrainWorker pid=19921, ip=10.0.55.91) {'loss': 0.9748, 'learning_rate': 0.0009428571428571429, 'epoch': 0.11}\n",
      "(RayTrainWorker pid=19921, ip=10.0.55.91) {'loss': 1.0321, 'learning_rate': 0.0009408163265306123, 'epoch': 0.12}\n",
      "(RayTrainWorker pid=13353, ip=10.0.15.54) {'loss': 1.0321, 'learning_rate': 0.0009408163265306123, 'epoch': 0.12}\n",
      "(RayTrainWorker pid=19921, ip=10.0.55.91) {'loss': 0.9521, 'learning_rate': 0.0009387755102040817, 'epoch': 0.12}\n",
      "(RayTrainWorker pid=13353, ip=10.0.15.54) {'loss': 0.9521, 'learning_rate': 0.0009387755102040817, 'epoch': 0.12}\n",
      "(RayTrainWorker pid=13353, ip=10.0.15.54) {'loss': 0.9089, 'learning_rate': 0.0009367346938775511, 'epoch': 0.13}\n",
      "(RayTrainWorker pid=19921, ip=10.0.55.91) {'loss': 0.9089, 'learning_rate': 0.0009367346938775511, 'epoch': 0.13}\n"
     ]
    }
   ],
   "source": [
    "results = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.air import Checkpoint\n",
    "checkpoint = Checkpoint.from_directory(\"/mnt/cluster_storage/HuggingFaceTrainer_2023-02-08_14-55-52/HuggingFaceTrainer_bca80_00000_0_2023-02-08_14-55-53/rank_0/gpt-j-6B/checkpoint-394/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train.huggingface import HuggingFaceCheckpoint\n",
    "from ray.air._internal.checkpointing import (\n",
    "    load_preprocessor_from_dir,\n",
    "    save_preprocessor_to_dir,\n",
    ")\n",
    "\n",
    "class HuggingFaceCheckpointPatched(HuggingFaceCheckpoint):\n",
    "    def get_preprocessor(self):\n",
    "        \"\"\"Return the saved preprocessor, if one exists.\"\"\"\n",
    "\n",
    "        # The preprocessor will either be stored in an in-memory dict or\n",
    "        # written to storage. In either case, it will use the PREPROCESSOR_KEY key.\n",
    "\n",
    "        with self.as_directory() as checkpoint_path:\n",
    "            preprocessor = load_preprocessor_from_dir(checkpoint_path)\n",
    "\n",
    "        return preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train.huggingface import HuggingFacePredictor\n",
    "from transformers import set_seed\n",
    "\n",
    "@ray.remote(num_gpus=1)\n",
    "def predict(uri, seed=None):\n",
    "    if seed is None:\n",
    "        rng = np.random.default_rng(seed=None)\n",
    "        seed = rng.integers(0, 2**16)\n",
    "    print(f\"seed: {seed}\")\n",
    "    set_seed(seed)\n",
    "    checkpoint = HuggingFaceCheckpointPatched.from_uri(uri)\n",
    "    print(\"creating predictor\")\n",
    "    predictor = HuggingFacePredictor.from_checkpoint(checkpoint, task=\"text-generation\", device=0, torch_dtype=torch.bfloat16)\n",
    "    # No need to use AIR preprocessor, and it looks like the one I coded has\n",
    "    # issues with being loaded, so we just get rid of it\n",
    "    predictor._preprocessor = None\n",
    "    print(\"predicting\")\n",
    "    return predictor.predict(\n",
    "        pd.DataFrame([[\"Romeo:\"]]),\n",
    "        do_sample=True, \n",
    "        max_new_tokens=256, \n",
    "        top_k=50, \n",
    "        top_p=0.95, \n",
    "        num_return_sequences=3\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_tasks = [predict.remote(checkpoint.uri) for i in range(8)]\n",
    "predictions = ray.get(prediction_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
