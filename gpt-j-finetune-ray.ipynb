{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "comet_ml is installed but `COMET_API_KEY` is not set.\n",
      "/home/ray/anaconda3/lib/python3.8/site-packages/xgboost/compat.py:31: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "2023-02-08 19:29:47,979\tINFO worker.py:1364 -- Connecting to existing Ray cluster at address: 10.0.11.234:6379...\n",
      "2023-02-08 19:29:47,989\tINFO worker.py:1544 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2023-02-08 19:29:47,993\tINFO packaging.py:330 -- Pushing file package 'gcs://_ray_pkg_f2e93942ab6ad3d2e347b209b7e2bae2.zip' (0.37MiB) to Ray cluster...\n",
      "2023-02-08 19:29:47,997\tINFO packaging.py:343 -- Successfully pushed file package 'gcs://_ray_pkg_f2e93942ab6ad3d2e347b209b7e2bae2.zip'.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, AutoModelForCausalLM\n",
    "from transformers import GPTJForCausalLM, AutoTokenizer, DataCollatorForLanguageModeling, default_data_collator\n",
    "from datasets import load_dataset\n",
    "import time\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import ray\n",
    "import ray.data\n",
    "from ray.data.preprocessors import BatchMapper, Chain\n",
    "import os\n",
    "#os.environ[\"RAY_ML_DEV\"] = \"1\"\n",
    "\n",
    "\n",
    "\n",
    "ray.init(runtime_env={\"env_vars\": {\"NCCL_SOCKET_IFNAME\": \"ens5\"}})\n",
    "start = time.time()\n",
    "name = \"gpt-j-6B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset tiny_shakespeare (/home/ray/.cache/huggingface/datasets/tiny_shakespeare/default/1.0.0/b5b13969f09fe8707337f6cb296314fbe06960bd9a868dca39e713e163d27b5e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7668bfb074cc4621883e86c1a9461462",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Loading dataset\")\n",
    "# current_dataset = load_dataset(\"wikitext\", 'wikitext-103-v1', split=\"train\")\n",
    "current_dataset = load_dataset(\"tiny_shakespeare\")\n",
    "current_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(current_dataset, dict):\n",
    "    ray_datasets = ray.data.from_huggingface(current_dataset)\n",
    "else:\n",
    "    ray_dataset: ray.data.Dataset = ray.data.from_huggingface(current_dataset)\n",
    "    train, validation, test = ray_dataset.random_shuffle(seed=1).split_proportionately([0.9])\n",
    "    ray_datasets = {\"train\": train.repartition(16), \"validation\": validation.repartition(4)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_column_with_one_string(df):\n",
    "    data = df[\"text\"].iloc[0]\n",
    "    df = pd.DataFrame()\n",
    "    df[\"text\"] = [x.strip() for x in data.split(\"\\n\\n\") if x.strip()]\n",
    "    return df\n",
    "\n",
    "string_splitter = BatchMapper(split_column_with_one_string, batch_format=\"pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.data.preprocessor import Preprocessor\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, pretrained_model_name_or_path, caption_column, revision=None) -> None:\n",
    "        # Importing here to work around a memory leak with Ray Data in 2.2\n",
    "        # Should be fixed in 2.3 or 2.4\n",
    "        from transformers import AutoTokenizer\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path, revision=revision)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.caption_column = caption_column\n",
    "\n",
    "    # Preprocessing the datasets.\n",
    "    # We need to tokenize input captions and transform the images.\n",
    "    def tokenize_captions(self, txt_list, is_train=True):\n",
    "        tokens = self.tokenizer(list(txt_list[self.caption_column]), truncation=True,\n",
    "                                       max_length=self.tokenizer.model_max_length, padding=\"max_length\",return_tensors=\"np\",)        \n",
    "        tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
    "        return {k: v for k, v in tokens.items()}\n",
    "\n",
    "    def __call__(self, df: \"pd.DataFrame\") -> \"pd.DataFrame\":\n",
    "        return self.tokenize_captions(df)\n",
    "\n",
    "\n",
    "class TokenizerPreprocessor(Preprocessor):\n",
    "    _is_fittable = False\n",
    "\n",
    "    def __init__(self, pretrained_model_name_or_path, caption_column, revision=None) -> None:\n",
    "        self.pretrained_model_name_or_path = pretrained_model_name_or_path\n",
    "        self.revision = revision\n",
    "        self.caption_column = caption_column\n",
    "\n",
    "    _transform_pandas = Tokenizer\n",
    "\n",
    "    def _get_transform_config(self):\n",
    "        \"\"\"Returns kwargs to be passed to :meth:`ray.data.Dataset.map_batches`.\n",
    "        This can be implemented by subclassing preprocessors.\n",
    "        \"\"\"\n",
    "        return dict(\n",
    "            compute=ray.data.ActorPoolStrategy(),\n",
    "            fn_constructor_kwargs=dict(\n",
    "                pretrained_model_name_or_path=self.pretrained_model_name_or_path,\n",
    "                revision=self.revision,\n",
    "                caption_column=self.caption_column,\n",
    "            ),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.utils.logging import disable_progress_bar, enable_progress_bar\n",
    "from ray.air import session\n",
    "import torch\n",
    "import os\n",
    "\n",
    "num_cpus = 8\n",
    "\n",
    "def trainer_init_per_worker(train_dataset, eval_dataset = None, **config):\n",
    "    os.environ[\"OMP_NUM_THREADS\"] = str(num_cpus)\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "    batch_size = 4\n",
    "    deepspeed = {\n",
    "        \"fp16\": {\n",
    "            \"enabled\": \"auto\",\n",
    "            \"initial_scale_power\": 32,\n",
    "        },\n",
    "        \"bf16\":{\n",
    "            \"enabled\":\"auto\"\n",
    "        },    \n",
    "        \"optimizer\": {\n",
    "            \"type\": \"AdamW\",\n",
    "            \"params\": {\n",
    "            \"lr\": \"auto\",\n",
    "            \"betas\": \"auto\",\n",
    "            \"eps\":\"auto\",\n",
    "            }\n",
    "        },\n",
    "        \"zero_optimization\": {\n",
    "            \"stage\": 3,\n",
    "            \"offload_optimizer\": {\n",
    "                \"device\": \"cpu\",\n",
    "                \"pin_memory\": False,\n",
    "            },\n",
    "           # \"offload_param\": {\n",
    "           #     \"device\": \"cpu\",\n",
    "           #     \"pin_memory\": False,\n",
    "           # },\n",
    "            \"overlap_comm\": True,\n",
    "            \"contiguous_gradients\": True,\n",
    "            \"reduce_bucket_size\": \"auto\",\n",
    "            \"stage3_prefetch_bucket_size\": \"auto\",\n",
    "            \"stage3_param_persistence_threshold\": \"auto\",\n",
    "            \"gather_16bit_weights_on_model_save\": True,\n",
    "            \"round_robin_gradients\": True,\n",
    "        },\n",
    "        \"gradient_accumulation_steps\": \"auto\",\n",
    "        \"gradient_clipping\": \"auto\",\n",
    "        \"steps_per_print\": 1,\n",
    "        \"train_batch_size\": \"auto\",\n",
    "        \"train_micro_batch_size_per_gpu\": \"auto\",\n",
    "        \"wall_clock_breakdown\": False\n",
    "    }\n",
    "\n",
    "    print(\"Preparing training arguments\")\n",
    "    training_args = TrainingArguments(\n",
    "        name,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        logging_steps=100, save_strategy=\"steps\",\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        learning_rate=0.001,\n",
    "        weight_decay=0.01,\n",
    "        # warmup_steps=20,\n",
    "        label_names=['input_ids', 'attention_mask'],  # 'logits', 'past_key_values'\n",
    "        num_train_epochs=config.get(\"epochs\", 2),\n",
    "        push_to_hub=False,\n",
    "        disable_tqdm=True,  # declutter the output a little\n",
    "        bf16=True,\n",
    "        gradient_checkpointing=True,\n",
    "        deepspeed=deepspeed\n",
    "    )\n",
    "\n",
    "    disable_progress_bar()\n",
    "    print(\"Loading model\")\n",
    "    model = GPTJForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\", use_cache=False, revision=\"float16\", torch_dtype=torch.float16)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    enable_progress_bar()\n",
    "\n",
    "    metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        predictions = np.argmax(logits, axis=-1)\n",
    "        return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=default_data_collator,\n",
    "    )\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train.huggingface import HuggingFaceTrainer\n",
    "from ray.air.config import RunConfig, ScalingConfig, CheckpointConfig\n",
    "from ray.air.integrations.mlflow import MLflowLoggerCallback\n",
    "from ray.tune import SyncConfig\n",
    "\n",
    "trainer = HuggingFaceTrainer(\n",
    "    trainer_init_per_worker=trainer_init_per_worker,\n",
    "    scaling_config=ScalingConfig(num_workers=8, use_gpu=True, resources_per_worker={\"GPU\": 1, \"CPU\": num_cpus}),\n",
    "    datasets={\"train\": ray_datasets[\"train\"], \"evaluation\": ray_datasets[\"validation\"]},\n",
    "    run_config=RunConfig(\n",
    "        local_dir=\"/mnt/cluster_storage/\",\n",
    "        sync_config=SyncConfig(syncer=None),\n",
    "        callbacks=[MLflowLoggerCallback(experiment_name=name)],\n",
    "        checkpoint_config=CheckpointConfig(num_to_keep=1, checkpoint_score_attribute=\"eval_loss\", checkpoint_score_order=\"min\"),\n",
    "    ),\n",
    "    preprocessor=Chain(string_splitter, TokenizerPreprocessor(\"EleutherAI/gpt-j-6B\", \"text\")),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.air import Checkpoint\n",
    "checkpoint = Checkpoint.from_directory(\"/mnt/cluster_storage/HuggingFaceTrainer_2023-02-08_14-55-52/HuggingFaceTrainer_bca80_00000_0_2023-02-08_14-55-53/rank_0/gpt-j-6B/checkpoint-394/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Checkpoint(local_path=/efs/workspaces/expwrk_pz38qgkkv829gd4lz5k5zisa9g/cluster_storage/HuggingFaceTrainer_2023-02-08_14-55-52/HuggingFaceTrainer_bca80_00000_0_2023-02-08_14-55-53/rank_0/gpt-j-6B/checkpoint-394)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train.huggingface import HuggingFaceCheckpoint\n",
    "from ray.air._internal.checkpointing import (\n",
    "    load_preprocessor_from_dir,\n",
    "    save_preprocessor_to_dir,\n",
    ")\n",
    "\n",
    "class HuggingFaceCheckpointPatched(HuggingFaceCheckpoint):\n",
    "    def get_preprocessor(self):\n",
    "        \"\"\"Return the saved preprocessor, if one exists.\"\"\"\n",
    "\n",
    "        # The preprocessor will either be stored in an in-memory dict or\n",
    "        # written to storage. In either case, it will use the PREPROCESSOR_KEY key.\n",
    "\n",
    "        with self.as_directory() as checkpoint_path:\n",
    "            preprocessor = load_preprocessor_from_dir(checkpoint_path)\n",
    "\n",
    "        return preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train.huggingface import HuggingFacePredictor\n",
    "from transformers import set_seed\n",
    "\n",
    "@ray.remote(num_gpus=1)\n",
    "def predict(uri, seed=None):\n",
    "    if seed is None:\n",
    "        rng = np.random.default_rng(seed=None)\n",
    "        seed = rng.integers(0, 2**16)\n",
    "    print(f\"seed: {seed}\")\n",
    "    set_seed(seed)\n",
    "    checkpoint = HuggingFaceCheckpointPatched.from_uri(uri)\n",
    "    print(\"creating predictor\")\n",
    "    predictor = HuggingFacePredictor.from_checkpoint(checkpoint, task=\"text-generation\", device=0, torch_dtype=torch.bfloat16)\n",
    "    # No need to use AIR preprocessor, and it looks like the one I coded has\n",
    "    # issues with being loaded, so we just get rid of it\n",
    "    predictor._preprocessor = None\n",
    "    print(\"predicting\")\n",
    "    return predictor.predict(\n",
    "        pd.DataFrame([[\"Romeo:\"]]),\n",
    "        do_sample=True, \n",
    "        max_new_tokens=256, \n",
    "        top_k=50, \n",
    "        top_p=0.95, \n",
    "        num_return_sequences=3\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(pid=729, ip=10.0.61.192) /home/ray/anaconda3/lib/python3.8/site-packages/xgboost/compat.py:31: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "(pid=729, ip=10.0.61.192)   from pandas import MultiIndex, Int64Index\n",
      "(pid=3174, ip=10.0.16.95) /home/ray/anaconda3/lib/python3.8/site-packages/xgboost/compat.py:31: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "(pid=3174, ip=10.0.16.95)   from pandas import MultiIndex, Int64Index\n",
      "(pid=736, ip=10.0.2.170) /home/ray/anaconda3/lib/python3.8/site-packages/xgboost/compat.py:31: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "(pid=736, ip=10.0.2.170)   from pandas import MultiIndex, Int64Index\n",
      "(pid=729, ip=10.0.60.56) /home/ray/anaconda3/lib/python3.8/site-packages/xgboost/compat.py:31: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "(pid=729, ip=10.0.60.56)   from pandas import MultiIndex, Int64Index\n",
      "(pid=728, ip=10.0.42.1) /home/ray/anaconda3/lib/python3.8/site-packages/xgboost/compat.py:31: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "(pid=728, ip=10.0.42.1)   from pandas import MultiIndex, Int64Index\n",
      "(pid=3174, ip=10.0.16.95) comet_ml is installed but `COMET_API_KEY` is not set.\n",
      "(pid=729, ip=10.0.61.192) comet_ml is installed but `COMET_API_KEY` is not set.\n",
      "(pid=485, ip=10.0.59.166) /home/ray/anaconda3/lib/python3.8/site-packages/xgboost/compat.py:31: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "(pid=485, ip=10.0.59.166)   from pandas import MultiIndex, Int64Index\n",
      "(pid=736, ip=10.0.2.170) comet_ml is installed but `COMET_API_KEY` is not set.\n",
      "(pid=729, ip=10.0.60.56) comet_ml is installed but `COMET_API_KEY` is not set.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(predict pid=3174, ip=10.0.16.95) seed: 36085\n",
      "(predict pid=3174, ip=10.0.16.95) creating predictor\n",
      "(predict pid=729, ip=10.0.61.192) seed: 5197\n",
      "(predict pid=729, ip=10.0.61.192) creating predictor\n",
      "(predict pid=736, ip=10.0.2.170) seed: 40401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(pid=485, ip=10.0.12.116) /home/ray/anaconda3/lib/python3.8/site-packages/xgboost/compat.py:31: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "(pid=485, ip=10.0.12.116)   from pandas import MultiIndex, Int64Index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(predict pid=736, ip=10.0.2.170) creating predictor\n",
      "(predict pid=729, ip=10.0.60.56) seed: 11975\n",
      "(predict pid=729, ip=10.0.60.56) creating predictor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(pid=728, ip=10.0.42.1) comet_ml is installed but `COMET_API_KEY` is not set.\n",
      "(pid=486, ip=10.0.11.114) /home/ray/anaconda3/lib/python3.8/site-packages/xgboost/compat.py:31: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "(pid=486, ip=10.0.11.114)   from pandas import MultiIndex, Int64Index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(predict pid=728, ip=10.0.42.1) seed: 22916\n",
      "(predict pid=728, ip=10.0.42.1) creating predictor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(pid=485, ip=10.0.59.166) comet_ml is installed but `COMET_API_KEY` is not set.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(predict pid=485, ip=10.0.59.166) seed: 63324\n",
      "(predict pid=485, ip=10.0.59.166) creating predictor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(pid=485, ip=10.0.12.116) comet_ml is installed but `COMET_API_KEY` is not set.\n",
      "(pid=486, ip=10.0.11.114) comet_ml is installed but `COMET_API_KEY` is not set.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(predict pid=485, ip=10.0.12.116) seed: 29835\n",
      "(predict pid=485, ip=10.0.12.116) creating predictor\n",
      "(predict pid=486, ip=10.0.11.114) seed: 25711\n",
      "(predict pid=486, ip=10.0.11.114) creating predictor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(predict pid=729, ip=10.0.61.192) 2023-02-08 19:45:15,253\tWARNING huggingface_predictor.py:71 -- You have `use_gpu` as False but there are 1 GPUs detected on host where prediction will only use CPU. Please consider explicitly setting `HuggingFacePredictor(use_gpu=True)` or `batch_predictor.predict(ds, num_gpus_per_worker=1)` to enable GPU prediction.\n",
      "(predict pid=729, ip=10.0.61.192) /mnt/cluster_storage/pypi/lib/python3.8/site-packages/transformers/generation/utils.py:1186: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "(predict pid=729, ip=10.0.61.192)   warnings.warn(\n",
      "(predict pid=729, ip=10.0.61.192) Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(predict pid=729, ip=10.0.61.192) predicting\n",
      "(predict pid=736, ip=10.0.2.170) predicting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(predict pid=736, ip=10.0.2.170) 2023-02-08 19:45:15,811\tWARNING huggingface_predictor.py:71 -- You have `use_gpu` as False but there are 1 GPUs detected on host where prediction will only use CPU. Please consider explicitly setting `HuggingFacePredictor(use_gpu=True)` or `batch_predictor.predict(ds, num_gpus_per_worker=1)` to enable GPU prediction.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(predict pid=3174, ip=10.0.16.95) predicting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(predict pid=3174, ip=10.0.16.95) 2023-02-08 19:45:15,721\tWARNING huggingface_predictor.py:71 -- You have `use_gpu` as False but there are 1 GPUs detected on host where prediction will only use CPU. Please consider explicitly setting `HuggingFacePredictor(use_gpu=True)` or `batch_predictor.predict(ds, num_gpus_per_worker=1)` to enable GPU prediction.\n",
      "(predict pid=3174, ip=10.0.16.95) /mnt/cluster_storage/pypi/lib/python3.8/site-packages/transformers/generation/utils.py:1186: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "(predict pid=3174, ip=10.0.16.95)   warnings.warn(\n",
      "(predict pid=3174, ip=10.0.16.95) Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "(predict pid=736, ip=10.0.2.170) /mnt/cluster_storage/pypi/lib/python3.8/site-packages/transformers/generation/utils.py:1186: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "(predict pid=736, ip=10.0.2.170)   warnings.warn(\n",
      "(predict pid=736, ip=10.0.2.170) Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(predict pid=729, ip=10.0.60.56) predicting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(predict pid=729, ip=10.0.60.56) 2023-02-08 19:45:16,647\tWARNING huggingface_predictor.py:71 -- You have `use_gpu` as False but there are 1 GPUs detected on host where prediction will only use CPU. Please consider explicitly setting `HuggingFacePredictor(use_gpu=True)` or `batch_predictor.predict(ds, num_gpus_per_worker=1)` to enable GPU prediction.\n",
      "(predict pid=729, ip=10.0.60.56) /mnt/cluster_storage/pypi/lib/python3.8/site-packages/transformers/generation/utils.py:1186: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "(predict pid=729, ip=10.0.60.56)   warnings.warn(\n",
      "(predict pid=729, ip=10.0.60.56) Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(predict pid=728, ip=10.0.42.1) predicting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(predict pid=728, ip=10.0.42.1) 2023-02-08 19:45:17,744\tWARNING huggingface_predictor.py:71 -- You have `use_gpu` as False but there are 1 GPUs detected on host where prediction will only use CPU. Please consider explicitly setting `HuggingFacePredictor(use_gpu=True)` or `batch_predictor.predict(ds, num_gpus_per_worker=1)` to enable GPU prediction.\n",
      "(predict pid=728, ip=10.0.42.1) /mnt/cluster_storage/pypi/lib/python3.8/site-packages/transformers/generation/utils.py:1186: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "(predict pid=728, ip=10.0.42.1)   warnings.warn(\n",
      "(predict pid=728, ip=10.0.42.1) Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(predict pid=485, ip=10.0.59.166) predicting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(predict pid=485, ip=10.0.59.166) 2023-02-08 19:47:11,923\tWARNING huggingface_predictor.py:71 -- You have `use_gpu` as False but there are 1 GPUs detected on host where prediction will only use CPU. Please consider explicitly setting `HuggingFacePredictor(use_gpu=True)` or `batch_predictor.predict(ds, num_gpus_per_worker=1)` to enable GPU prediction.\n",
      "(predict pid=485, ip=10.0.59.166) /mnt/cluster_storage/pypi/lib/python3.8/site-packages/transformers/generation/utils.py:1186: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "(predict pid=485, ip=10.0.59.166)   warnings.warn(\n",
      "(predict pid=485, ip=10.0.59.166) Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(predict pid=486, ip=10.0.11.114) predicting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(predict pid=486, ip=10.0.11.114) 2023-02-08 19:47:14,789\tWARNING huggingface_predictor.py:71 -- You have `use_gpu` as False but there are 1 GPUs detected on host where prediction will only use CPU. Please consider explicitly setting `HuggingFacePredictor(use_gpu=True)` or `batch_predictor.predict(ds, num_gpus_per_worker=1)` to enable GPU prediction.\n",
      "(predict pid=486, ip=10.0.11.114) /mnt/cluster_storage/pypi/lib/python3.8/site-packages/transformers/generation/utils.py:1186: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "(predict pid=486, ip=10.0.11.114)   warnings.warn(\n",
      "(predict pid=486, ip=10.0.11.114) Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(predict pid=485, ip=10.0.12.116) predicting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(predict pid=485, ip=10.0.12.116) 2023-02-08 19:47:30,654\tWARNING huggingface_predictor.py:71 -- You have `use_gpu` as False but there are 1 GPUs detected on host where prediction will only use CPU. Please consider explicitly setting `HuggingFacePredictor(use_gpu=True)` or `batch_predictor.predict(ds, num_gpus_per_worker=1)` to enable GPU prediction.\n",
      "(predict pid=485, ip=10.0.12.116) /mnt/cluster_storage/pypi/lib/python3.8/site-packages/transformers/generation/utils.py:1186: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "(predict pid=485, ip=10.0.12.116)   warnings.warn(\n",
      "(predict pid=485, ip=10.0.12.116) Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "prediction_tasks = [predict.remote(checkpoint.uri) for i in range(8)]\n",
    "predictions = ray.get(prediction_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[                                      generated_text\n",
       " 0                             Romeo:\\nI, it;, be at.\n",
       " 1  Romeo:\\nAnd,?\\nNow\\nWhat thee be one all a lor...\n",
       " 2                                        Romeo:\\nI:.,\n",
       "                                       generated_text\n",
       " 0  Romeo:\\nAnd the thy son and a I of no father o...\n",
       " 1  Romeo:\\nThe me,, of my love's not and my a not...\n",
       " 2                             Romeo:\\nHI, thou love.,\n",
       "                                       generated_text\n",
       " 0                         Romeo:\\nThat do you's you!\n",
       " 1  Romeo:\\nTo my other her of this her shall but ...\n",
       " 2                        Romeo:\\nIishness in a king.,\n",
       "          generated_text\n",
       " 0   Romeo:\\nHow of me,.\n",
       " 1      Romeo:\\nTo I it.\n",
       " 2  Romeo:\\nBut have of.,\n",
       "                                       generated_text\n",
       " 0     Romeo:\\nAs will in the man and my heart\\nNow,.\n",
       " 1                     Romeo:\\nI's a-ay, it, thy man.\n",
       " 2  Romeo:\\nThe more dto the love,\\nIly my a more ...,\n",
       "                                       generated_text\n",
       " 0  Romeo:\\nT'ow thet this you\\nAs as have, is and...\n",
       " 1                                             Romeo:\n",
       " 2  Romeo:\\nTh'est, in our life: I, I would with t...,\n",
       "                                       generated_text\n",
       " 0                                Romeo:\\nI, be a me.\n",
       " 1  Romeo:\\nBut\\nMyio, thy father; my not thy thy ...\n",
       " 2  Romeo:\\nFor not to him's it's not, I,\\nTh'mst ...,\n",
       "                                       generated_text\n",
       " 0  Romeo:\\nThe hands I, of myt's the good your lo...\n",
       " 1  Romeo:\\nI'll a father,,; so to it; I's not not...\n",
       " 2      Romeo:\\nTo you'd\\nAnd thy tongue I, of death.]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
