{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "comet_ml is installed but `COMET_API_KEY` is not set.\n",
      "/home/ray/anaconda3/lib/python3.8/site-packages/xgboost/compat.py:31: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "2023-02-09 14:50:46,570\tINFO worker.py:1364 -- Connecting to existing Ray cluster at address: 10.0.48.108:6379...\n",
      "2023-02-09 14:50:46,580\tINFO worker.py:1544 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2023-02-09 14:50:46,583\tINFO packaging.py:330 -- Pushing file package 'gcs://_ray_pkg_1e3dac5337413e0660dd7da00d1f0b6e.zip' (0.13MiB) to Ray cluster...\n",
      "2023-02-09 14:50:46,585\tINFO packaging.py:343 -- Successfully pushed file package 'gcs://_ray_pkg_1e3dac5337413e0660dd7da00d1f0b6e.zip'.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, AutoModelForCausalLM\n",
    "from transformers import GPTJForCausalLM, AutoTokenizer, DataCollatorForLanguageModeling, default_data_collator\n",
    "from datasets import load_dataset\n",
    "import time\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import ray\n",
    "import ray.data\n",
    "from ray.data.preprocessors import BatchMapper, Chain\n",
    "import os\n",
    "#os.environ[\"RAY_ML_DEV\"] = \"1\"\n",
    "\n",
    "\n",
    "\n",
    "ray.init(runtime_env={\"env_vars\": {\"NCCL_SOCKET_IFNAME\": \"ens5\"}})\n",
    "start = time.time()\n",
    "name = \"gpt-j-6B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset tiny_shakespeare (/home/ray/.cache/huggingface/datasets/tiny_shakespeare/default/1.0.0/b5b13969f09fe8707337f6cb296314fbe06960bd9a868dca39e713e163d27b5e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d27d4df15f454daeb4f7666092ac1ae1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Loading dataset\")\n",
    "# current_dataset = load_dataset(\"wikitext\", 'wikitext-103-v1', split=\"train\")\n",
    "current_dataset = load_dataset(\"tiny_shakespeare\")\n",
    "current_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(current_dataset, dict):\n",
    "    ray_datasets = ray.data.from_huggingface(current_dataset)\n",
    "else:\n",
    "    ray_dataset: ray.data.Dataset = ray.data.from_huggingface(current_dataset)\n",
    "    train, validation, test = ray_dataset.random_shuffle(seed=1).split_proportionately([0.9])\n",
    "    ray_datasets = {\"train\": train.repartition(16), \"validation\": validation.repartition(4)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 1024\n",
    "def split_column_with_one_string(df):\n",
    "    data = df[\"text\"].iloc[0]\n",
    "    df = pd.DataFrame()\n",
    "    #df[\"text\"] = [x.strip() for x in data.split(\"\\n\\n\") if x.strip()]\n",
    "    df[\"text\"] = [data[i:i+block_size].strip() for i in range(0, len(data), block_size)]\n",
    "    return df\n",
    "\n",
    "string_splitter = BatchMapper(split_column_with_one_string, batch_format=\"pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.data.preprocessor import Preprocessor\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, pretrained_model_name_or_path, caption_column, revision=None) -> None:\n",
    "        # Importing here to work around a memory leak with Ray Data in 2.2\n",
    "        # Should be fixed in 2.3 or 2.4\n",
    "        from transformers import AutoTokenizer\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path, revision=revision)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.caption_column = caption_column\n",
    "\n",
    "    # Preprocessing the datasets.\n",
    "    # We need to tokenize input captions and transform the images.\n",
    "    def tokenize_captions(self, txt_list, is_train=True):\n",
    "        tokens = self.tokenizer(list(txt_list[self.caption_column]), truncation=True,\n",
    "                                       max_length=self.tokenizer.model_max_length, padding=\"max_length\",return_tensors=\"np\",)        \n",
    "        tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
    "        return {k: v for k, v in tokens.items()}\n",
    "\n",
    "    def __call__(self, df: \"pd.DataFrame\") -> \"pd.DataFrame\":\n",
    "        return self.tokenize_captions(df)\n",
    "\n",
    "\n",
    "class TokenizerPreprocessor(Preprocessor):\n",
    "    _is_fittable = False\n",
    "\n",
    "    def __init__(self, pretrained_model_name_or_path, caption_column, revision=None) -> None:\n",
    "        self.pretrained_model_name_or_path = pretrained_model_name_or_path\n",
    "        self.revision = revision\n",
    "        self.caption_column = caption_column\n",
    "\n",
    "    _transform_pandas = Tokenizer\n",
    "\n",
    "    def _get_transform_config(self):\n",
    "        \"\"\"Returns kwargs to be passed to :meth:`ray.data.Dataset.map_batches`.\n",
    "        This can be implemented by subclassing preprocessors.\n",
    "        \"\"\"\n",
    "        return dict(\n",
    "            compute=ray.data.ActorPoolStrategy(),\n",
    "            fn_constructor_kwargs=dict(\n",
    "                pretrained_model_name_or_path=self.pretrained_model_name_or_path,\n",
    "                revision=self.revision,\n",
    "                caption_column=self.caption_column,\n",
    "            ),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.utils.logging import disable_progress_bar, enable_progress_bar\n",
    "from transformers.utils.hub import cached_file\n",
    "from accelerate.big_modeling import get_balanced_memory, infer_auto_device_map, load_checkpoint_and_dispatch, init_empty_weights\n",
    "from deepspeed.ops.adam.cpu_adam import DeepSpeedCPUAdam\n",
    "from ray.air import session\n",
    "import torch\n",
    "import os\n",
    "from transformers.trainer_pt_utils import get_parameter_names\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "num_cpus = 8\n",
    "\n",
    "class TrainingArgumentsPatched(TrainingArguments):\n",
    "    @property\n",
    "    def place_model_on_device(self):\n",
    "        return False\n",
    "\n",
    "def trainer_init_per_worker(train_dataset, eval_dataset = None, **config):\n",
    "    # Env vars necessary for HF to setup DDP\n",
    "    #os.environ.pop(\"RANK\")\n",
    "    #os.environ.pop(\"WORLD_SIZE\")\n",
    "    #os.environ.pop(\"LOCAL_RANK\")\n",
    "\n",
    "    os.environ[\"OMP_NUM_THREADS\"] = str(num_cpus)\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "    batch_size = 6\n",
    "    deepspeed = {\n",
    "        \"fp16\": {\n",
    "            \"enabled\": \"auto\",\n",
    "            \"initial_scale_power\": 32,\n",
    "        },\n",
    "        \"bf16\":{\n",
    "            \"enabled\":\"auto\"\n",
    "        },    \n",
    "        \"optimizer\": {\n",
    "            \"type\": \"AdamW\",\n",
    "            \"params\": {\n",
    "            \"lr\": \"auto\",\n",
    "            \"betas\": \"auto\",\n",
    "            \"eps\":\"auto\",\n",
    "            }\n",
    "        },\n",
    "        \"zero_optimization\": {\n",
    "            \"stage\": 3,\n",
    "            \"offload_optimizer\": {\n",
    "                \"device\": \"cpu\",\n",
    "                \"pin_memory\": False,\n",
    "            },\n",
    "           # \"offload_param\": {\n",
    "           #     \"device\": \"cpu\",\n",
    "           #     \"pin_memory\": False,\n",
    "           # },\n",
    "            \"overlap_comm\": True,\n",
    "            \"contiguous_gradients\": True,\n",
    "            \"reduce_bucket_size\": \"auto\",\n",
    "            \"stage3_prefetch_bucket_size\": \"auto\",\n",
    "            \"stage3_param_persistence_threshold\": \"auto\",\n",
    "            \"gather_16bit_weights_on_model_save\": True,\n",
    "            \"round_robin_gradients\": True,\n",
    "        },\n",
    "        \"gradient_accumulation_steps\": \"auto\",\n",
    "        \"gradient_clipping\": \"auto\",\n",
    "        \"steps_per_print\": 1,\n",
    "        \"train_batch_size\": \"auto\",\n",
    "        \"train_micro_batch_size_per_gpu\": \"auto\",\n",
    "        \"wall_clock_breakdown\": False\n",
    "    }\n",
    "    print(\"Preparing training arguments\")\n",
    "    training_args = TrainingArguments(\n",
    "        name,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        logging_steps=1, save_strategy=\"steps\",\n",
    "        save_steps=490,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        learning_rate=0.001,\n",
    "        weight_decay=0.01,\n",
    "        # warmup_steps=20,\n",
    "        label_names=['input_ids', 'attention_mask'],  # 'logits', 'past_key_values'\n",
    "        num_train_epochs=config.get(\"epochs\", 2),\n",
    "        push_to_hub=False,\n",
    "        disable_tqdm=True,  # declutter the output a little\n",
    "        bf16=True,\n",
    "        gradient_checkpointing=True,\n",
    "        #local_rank=-1,\n",
    "        deepspeed=deepspeed\n",
    "    )\n",
    "    disable_progress_bar()\n",
    "    dtype = torch.float32\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    print(\"Loading model\")\n",
    "\n",
    "    #with init_empty_weights():\n",
    "    model = GPTJForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\", use_cache=False)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    decay_parameters = get_parameter_names(model, [nn.LayerNorm])\n",
    "    decay_parameters = [name for name in decay_parameters if \"bias\" not in name]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if n in decay_parameters],\n",
    "            \"weight_decay\": training_args.weight_decay,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if n not in decay_parameters],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    optimizer = SM3(\n",
    "        optimizer_grouped_parameters,\n",
    "        beta=0,\n",
    "        eps=training_args.adam_epsilon,\n",
    "        lr=training_args.learning_rate,\n",
    "    )\n",
    "\n",
    "    # # Load the checkpoint and dispatch it to the right devices\n",
    "    # model = load_checkpoint_and_dispatch(\n",
    "    #     model,\n",
    "    #     cached_file(\"EleutherAI/gpt-j-6B\", \"pytorch_model.bin\", revision=\"float16\", local_files_only=True),\n",
    "    #     device_map=\"auto\",\n",
    "    #     no_split_module_classes=[\"GPTJBlock\"],\n",
    "    #     dtype=dtype,\n",
    "    # )\n",
    "\n",
    "    enable_progress_bar()\n",
    "\n",
    "    metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        predictions = np.argmax(logits, axis=-1)\n",
    "        return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=default_data_collator,\n",
    "        #optimizers=(optimizer, None)\n",
    "    )\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train.huggingface import HuggingFaceTrainer\n",
    "from ray.air.config import RunConfig, ScalingConfig, CheckpointConfig\n",
    "from ray.air.integrations.mlflow import MLflowLoggerCallback\n",
    "from ray.tune import SyncConfig\n",
    "\n",
    "class HuggingFaceTrainerPatched(HuggingFaceTrainer):\n",
    "    def _validate_attributes(self):\n",
    "        for key, conf in self._dataset_config.items():\n",
    "            if conf.use_stream_api:\n",
    "                raise ValueError(\n",
    "                    \"HuggingFaceTrainer does not support `use_stream_api`.\"\n",
    "                )\n",
    "        super(HuggingFaceTrainer, self)._validate_attributes()\n",
    "\n",
    "trainer = HuggingFaceTrainerPatched(\n",
    "    trainer_init_per_worker=trainer_init_per_worker,\n",
    "    scaling_config=ScalingConfig(num_workers=16, use_gpu=True, resources_per_worker={\"GPU\": 1, \"CPU\": 96/8}),\n",
    "    datasets={\"train\": ray_datasets[\"train\"], \"evaluation\": ray_datasets[\"validation\"]},\n",
    "    run_config=RunConfig(\n",
    "        local_dir=\"/mnt/cluster_storage/\",\n",
    "        sync_config=SyncConfig(syncer=None),\n",
    "        callbacks=[MLflowLoggerCallback(experiment_name=name)],\n",
    "        checkpoint_config=CheckpointConfig(num_to_keep=1, checkpoint_score_attribute=\"eval_loss\", checkpoint_score_order=\"min\"),\n",
    "    ),\n",
    "    preprocessor=Chain(string_splitter, TokenizerPreprocessor(\"EleutherAI/gpt-j-6B\", \"text\")),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-02-09 17:09:00</td></tr>\n",
       "<tr><td>Running for: </td><td>00:14:15.13        </td></tr>\n",
       "<tr><td>Memory:      </td><td>6.7/62.0 GiB       </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Resources requested: 0/400 CPUs, 0/16 GPUs, 0.0/1159.97 GiB heap, 0.0/390.94 GiB objects\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  \n",
       "  \n",
       "  Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                           </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                                              </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>HuggingFaceTrainerPatched_82a20_00000</td><td style=\"text-align: right;\">           1</td><td>/mnt/cluster_storage/HuggingFaceTrainerPatched_2023-02-09_16-54-45/HuggingFaceTrainerPatched_82a20_00000_0_2023-02-09_16-54-45/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                           </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  loss</th><th style=\"text-align: right;\">  learning_rate</th><th style=\"text-align: right;\">  epoch</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>HuggingFaceTrainerPatched_82a20_00000</td><td>ERROR   </td><td>10.0.55.91:74513</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         807.908</td><td style=\"text-align: right;\">0.9622</td><td style=\"text-align: right;\">    4.54545e-05</td><td style=\"text-align: right;\">1.90909</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-09 16:54:45,129\tWARNING trial_runner.py:369 -- The maximum number of pending trials has been automatically set to the number of available cluster CPUs, which is high (440 CPUs/pending trials). If you're running an experiment with a large number of trials, this could lead to scheduling overhead. In this case, consider setting the `TUNE_MAX_PENDING_TRIALS_PG` environment variable to the desired maximum number of concurrent trials.\n",
      "(pid=74513, ip=10.0.55.91) /home/ray/anaconda3/lib/python3.8/site-packages/xgboost/compat.py:31: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "(pid=74513, ip=10.0.55.91)   from pandas import MultiIndex, Int64Index\n",
      "(pid=74513, ip=10.0.55.91) comet_ml is installed but `COMET_API_KEY` is not set.\n",
      "(HuggingFaceTrainerPatched pid=74513, ip=10.0.55.91) 2023-02-09 16:54:57,638\tINFO bulk_executor.py:39 -- Executing DAG InputDataBuffer[Input] -> AllToAllOperator[randomize_block_order]\n",
      "(HuggingFaceTrainerPatched pid=74513, ip=10.0.55.91) 2023-02-09 16:54:57,641\tINFO bulk_executor.py:39 -- Executing DAG InputDataBuffer[Input] -> AllToAllOperator[randomize_block_order]\n",
      "(HuggingFaceTrainerPatched pid=74513, ip=10.0.55.91) 2023-02-09 16:54:57,648\tINFO bulk_executor.py:39 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[BatchMapper]\n",
      "(HuggingFaceTrainerPatched pid=74513, ip=10.0.55.91) 2023-02-09 16:54:57,685\tINFO bulk_executor.py:39 -- Executing DAG InputDataBuffer[Input] -> ActorPoolMapOperator[MapBatches(Tokenizer)]\n",
      "(HuggingFaceTrainerPatched pid=74513, ip=10.0.55.91) 2023-02-09 16:55:02,741\tINFO bulk_executor.py:39 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[BatchMapper]\n",
      "(HuggingFaceTrainerPatched pid=74513, ip=10.0.55.91) 2023-02-09 16:55:02,763\tINFO bulk_executor.py:39 -- Executing DAG InputDataBuffer[Input] -> ActorPoolMapOperator[MapBatches(Tokenizer)]\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) 2023-02-09 16:55:11,014\tINFO config.py:86 -- Setting up process group for: env:// [rank=0, world_size=16]\n",
      "(HuggingFaceTrainerPatched pid=74513, ip=10.0.55.91) /home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/bulk_dataset_iterator.py:108: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "(HuggingFaceTrainerPatched pid=74513, ip=10.0.55.91)   warnings.warn(\n",
      "(RayTrainWorker pid=66667, ip=10.0.15.54) /home/ray/anaconda3/lib/python3.8/site-packages/xgboost/compat.py:31: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "(RayTrainWorker pid=66667, ip=10.0.15.54)   from pandas import MultiIndex, Int64Index\n",
      "(RayTrainWorker pid=66677, ip=10.0.15.54) /home/ray/anaconda3/lib/python3.8/site-packages/xgboost/compat.py:31: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "(RayTrainWorker pid=66677, ip=10.0.15.54)   from pandas import MultiIndex, Int64Index\n",
      "(RayTrainWorker pid=75472, ip=10.0.55.91) /home/ray/anaconda3/lib/python3.8/site-packages/xgboost/compat.py:31: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "(RayTrainWorker pid=75472, ip=10.0.55.91)   from pandas import MultiIndex, Int64Index\n",
      "(RayTrainWorker pid=66669, ip=10.0.15.54) /home/ray/anaconda3/lib/python3.8/site-packages/xgboost/compat.py:31: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "(RayTrainWorker pid=66669, ip=10.0.15.54)   from pandas import MultiIndex, Int64Index\n",
      "(RayTrainWorker pid=75474, ip=10.0.55.91) /home/ray/anaconda3/lib/python3.8/site-packages/xgboost/compat.py:31: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "(RayTrainWorker pid=75474, ip=10.0.55.91)   from pandas import MultiIndex, Int64Index\n",
      "(RayTrainWorker pid=66668, ip=10.0.15.54) /home/ray/anaconda3/lib/python3.8/site-packages/xgboost/compat.py:31: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "(RayTrainWorker pid=66668, ip=10.0.15.54)   from pandas import MultiIndex, Int64Index\n",
      "(RayTrainWorker pid=66675, ip=10.0.15.54) /home/ray/anaconda3/lib/python3.8/site-packages/xgboost/compat.py:31: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "(RayTrainWorker pid=66675, ip=10.0.15.54)   from pandas import MultiIndex, Int64Index\n",
      "(RayTrainWorker pid=75476, ip=10.0.55.91) /home/ray/anaconda3/lib/python3.8/site-packages/xgboost/compat.py:31: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "(RayTrainWorker pid=75476, ip=10.0.55.91)   from pandas import MultiIndex, Int64Index\n",
      "(RayTrainWorker pid=66679, ip=10.0.15.54) /home/ray/anaconda3/lib/python3.8/site-packages/xgboost/compat.py:31: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "(RayTrainWorker pid=66679, ip=10.0.15.54)   from pandas import MultiIndex, Int64Index\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) /home/ray/anaconda3/lib/python3.8/site-packages/xgboost/compat.py:31: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)   from pandas import MultiIndex, Int64Index\n",
      "(RayTrainWorker pid=66670, ip=10.0.15.54) /home/ray/anaconda3/lib/python3.8/site-packages/xgboost/compat.py:31: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "(RayTrainWorker pid=66670, ip=10.0.15.54)   from pandas import MultiIndex, Int64Index\n",
      "(RayTrainWorker pid=75473, ip=10.0.55.91) /home/ray/anaconda3/lib/python3.8/site-packages/xgboost/compat.py:31: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "(RayTrainWorker pid=75473, ip=10.0.55.91)   from pandas import MultiIndex, Int64Index\n",
      "(RayTrainWorker pid=75471, ip=10.0.55.91) /home/ray/anaconda3/lib/python3.8/site-packages/xgboost/compat.py:31: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "(RayTrainWorker pid=75471, ip=10.0.55.91)   from pandas import MultiIndex, Int64Index\n",
      "(RayTrainWorker pid=75477, ip=10.0.55.91) /home/ray/anaconda3/lib/python3.8/site-packages/xgboost/compat.py:31: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "(RayTrainWorker pid=75477, ip=10.0.55.91)   from pandas import MultiIndex, Int64Index\n",
      "(RayTrainWorker pid=75479, ip=10.0.55.91) /home/ray/anaconda3/lib/python3.8/site-packages/xgboost/compat.py:31: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "(RayTrainWorker pid=75479, ip=10.0.55.91)   from pandas import MultiIndex, Int64Index\n",
      "(RayTrainWorker pid=66671, ip=10.0.15.54) /home/ray/anaconda3/lib/python3.8/site-packages/xgboost/compat.py:31: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "(RayTrainWorker pid=66671, ip=10.0.15.54)   from pandas import MultiIndex, Int64Index\n",
      "(RayTrainWorker pid=75472, ip=10.0.55.91) comet_ml is installed but `COMET_API_KEY` is not set.\n",
      "(RayTrainWorker pid=66669, ip=10.0.15.54) comet_ml is installed but `COMET_API_KEY` is not set.\n",
      "(RayTrainWorker pid=66677, ip=10.0.15.54) comet_ml is installed but `COMET_API_KEY` is not set.\n",
      "(RayTrainWorker pid=66667, ip=10.0.15.54) comet_ml is installed but `COMET_API_KEY` is not set.\n",
      "(RayTrainWorker pid=75474, ip=10.0.55.91) comet_ml is installed but `COMET_API_KEY` is not set.\n",
      "(RayTrainWorker pid=66675, ip=10.0.15.54) comet_ml is installed but `COMET_API_KEY` is not set.\n",
      "(RayTrainWorker pid=75476, ip=10.0.55.91) comet_ml is installed but `COMET_API_KEY` is not set.\n",
      "(RayTrainWorker pid=75473, ip=10.0.55.91) comet_ml is installed but `COMET_API_KEY` is not set.\n",
      "(RayTrainWorker pid=75471, ip=10.0.55.91) comet_ml is installed but `COMET_API_KEY` is not set.\n",
      "(RayTrainWorker pid=66670, ip=10.0.15.54) comet_ml is installed but `COMET_API_KEY` is not set.\n",
      "(RayTrainWorker pid=66668, ip=10.0.15.54) comet_ml is installed but `COMET_API_KEY` is not set.\n",
      "(RayTrainWorker pid=66679, ip=10.0.15.54) comet_ml is installed but `COMET_API_KEY` is not set.\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) comet_ml is installed but `COMET_API_KEY` is not set.\n",
      "(RayTrainWorker pid=75479, ip=10.0.55.91) comet_ml is installed but `COMET_API_KEY` is not set.\n",
      "(RayTrainWorker pid=66671, ip=10.0.15.54) comet_ml is installed but `COMET_API_KEY` is not set.\n",
      "(RayTrainWorker pid=75477, ip=10.0.55.91) comet_ml is installed but `COMET_API_KEY` is not set.\n",
      "(RayTrainWorker pid=66671, ip=10.0.15.54) /home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/bulk_dataset_iterator.py:108: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "(RayTrainWorker pid=66671, ip=10.0.15.54)   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=66668, ip=10.0.15.54) Preparing training arguments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=66675, ip=10.0.15.54) /home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/bulk_dataset_iterator.py:108: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "(RayTrainWorker pid=66675, ip=10.0.15.54)   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=66675, ip=10.0.15.54) Preparing training arguments\n",
      "(RayTrainWorker pid=66679, ip=10.0.15.54) Preparing training arguments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=66679, ip=10.0.15.54) /home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/bulk_dataset_iterator.py:108: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "(RayTrainWorker pid=66679, ip=10.0.15.54)   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=66667, ip=10.0.15.54) Preparing training arguments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=66667, ip=10.0.15.54) /home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/bulk_dataset_iterator.py:108: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "(RayTrainWorker pid=66667, ip=10.0.15.54)   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=66669, ip=10.0.15.54) Preparing training arguments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=66669, ip=10.0.15.54) /home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/bulk_dataset_iterator.py:108: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "(RayTrainWorker pid=66669, ip=10.0.15.54)   warnings.warn(\n",
      "(RayTrainWorker pid=66670, ip=10.0.15.54) /home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/bulk_dataset_iterator.py:108: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "(RayTrainWorker pid=66670, ip=10.0.15.54)   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=66670, ip=10.0.15.54) Preparing training arguments\n",
      "(RayTrainWorker pid=66677, ip=10.0.15.54) Preparing training arguments\n",
      "(RayTrainWorker pid=66671, ip=10.0.15.54) Preparing training arguments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=66677, ip=10.0.15.54) /home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/bulk_dataset_iterator.py:108: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "(RayTrainWorker pid=66677, ip=10.0.15.54)   warnings.warn(\n",
      "(RayTrainWorker pid=66668, ip=10.0.15.54) /home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/bulk_dataset_iterator.py:108: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "(RayTrainWorker pid=66668, ip=10.0.15.54)   warnings.warn(\n",
      "(RayTrainWorker pid=75472, ip=10.0.55.91) /home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/bulk_dataset_iterator.py:108: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "(RayTrainWorker pid=75472, ip=10.0.55.91)   warnings.warn(\n",
      "(RayTrainWorker pid=75477, ip=10.0.55.91) /home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/bulk_dataset_iterator.py:108: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "(RayTrainWorker pid=75477, ip=10.0.55.91)   warnings.warn(\n",
      "(RayTrainWorker pid=75476, ip=10.0.55.91) /home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/bulk_dataset_iterator.py:108: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "(RayTrainWorker pid=75476, ip=10.0.55.91)   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=75470, ip=10.0.55.91) Preparing training arguments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=75474, ip=10.0.55.91) /home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/bulk_dataset_iterator.py:108: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "(RayTrainWorker pid=75474, ip=10.0.55.91)   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=75473, ip=10.0.55.91) Preparing training arguments\n",
      "(RayTrainWorker pid=75474, ip=10.0.55.91) Preparing training arguments\n",
      "(RayTrainWorker pid=75476, ip=10.0.55.91) Preparing training arguments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=75470, ip=10.0.55.91) /home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/bulk_dataset_iterator.py:108: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=75471, ip=10.0.55.91) Preparing training arguments\n",
      "(RayTrainWorker pid=75472, ip=10.0.55.91) Preparing training arguments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=75473, ip=10.0.55.91) /home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/bulk_dataset_iterator.py:108: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "(RayTrainWorker pid=75473, ip=10.0.55.91)   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=75477, ip=10.0.55.91) Preparing training arguments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=75471, ip=10.0.55.91) /home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/bulk_dataset_iterator.py:108: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "(RayTrainWorker pid=75471, ip=10.0.55.91)   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=75479, ip=10.0.55.91) Preparing training arguments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=75479, ip=10.0.55.91) /home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/bulk_dataset_iterator.py:108: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "(RayTrainWorker pid=75479, ip=10.0.55.91)   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=66668, ip=10.0.15.54) Loading model\n",
      "(RayTrainWorker pid=66675, ip=10.0.15.54) Loading model\n",
      "(RayTrainWorker pid=66667, ip=10.0.15.54) Loading model\n",
      "(RayTrainWorker pid=66671, ip=10.0.15.54) Loading model\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) Loading model\n",
      "(RayTrainWorker pid=75473, ip=10.0.55.91) Loading model\n",
      "(RayTrainWorker pid=75474, ip=10.0.55.91) Loading model\n",
      "(RayTrainWorker pid=75476, ip=10.0.55.91) Loading model\n",
      "(RayTrainWorker pid=75471, ip=10.0.55.91) Loading model\n",
      "(RayTrainWorker pid=75472, ip=10.0.55.91) Loading model\n",
      "(RayTrainWorker pid=75477, ip=10.0.55.91) Loading model\n",
      "(RayTrainWorker pid=75479, ip=10.0.55.91) Loading model\n",
      "(RayTrainWorker pid=66679, ip=10.0.15.54) Loading model\n",
      "(RayTrainWorker pid=66669, ip=10.0.15.54) Loading model\n",
      "(RayTrainWorker pid=66670, ip=10.0.15.54) Loading model\n",
      "(RayTrainWorker pid=66677, ip=10.0.15.54) Loading model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=75477, ip=10.0.55.91) /home/ray/anaconda3/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "(RayTrainWorker pid=75477, ip=10.0.55.91)   warnings.warn(\n",
      "(RayTrainWorker pid=75479, ip=10.0.55.91) /home/ray/anaconda3/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "(RayTrainWorker pid=75479, ip=10.0.55.91)   warnings.warn(\n",
      "(RayTrainWorker pid=66671, ip=10.0.15.54) /home/ray/anaconda3/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "(RayTrainWorker pid=66671, ip=10.0.15.54)   warnings.warn(\n",
      "(RayTrainWorker pid=66675, ip=10.0.15.54) /home/ray/anaconda3/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "(RayTrainWorker pid=66675, ip=10.0.15.54)   warnings.warn(\n",
      "(RayTrainWorker pid=66679, ip=10.0.15.54) /home/ray/anaconda3/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "(RayTrainWorker pid=66679, ip=10.0.15.54)   warnings.warn(\n",
      "(RayTrainWorker pid=66667, ip=10.0.15.54) /home/ray/anaconda3/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "(RayTrainWorker pid=66667, ip=10.0.15.54)   warnings.warn(\n",
      "(RayTrainWorker pid=66669, ip=10.0.15.54) /home/ray/anaconda3/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "(RayTrainWorker pid=66669, ip=10.0.15.54)   warnings.warn(\n",
      "(RayTrainWorker pid=66670, ip=10.0.15.54) /home/ray/anaconda3/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "(RayTrainWorker pid=66670, ip=10.0.15.54)   warnings.warn(\n",
      "(RayTrainWorker pid=66677, ip=10.0.15.54) /home/ray/anaconda3/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "(RayTrainWorker pid=66677, ip=10.0.15.54)   warnings.warn(\n",
      "(RayTrainWorker pid=66668, ip=10.0.15.54) /home/ray/anaconda3/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "(RayTrainWorker pid=66668, ip=10.0.15.54)   warnings.warn(\n",
      "(RayTrainWorker pid=75472, ip=10.0.55.91) /home/ray/anaconda3/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "(RayTrainWorker pid=75472, ip=10.0.55.91)   warnings.warn(\n",
      "(RayTrainWorker pid=75476, ip=10.0.55.91) /home/ray/anaconda3/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "(RayTrainWorker pid=75476, ip=10.0.55.91)   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:55:33,325] [INFO] [partition_parameters.py:413:__exit__] finished initializing model with 6.05B parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=75474, ip=10.0.55.91) /home/ray/anaconda3/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "(RayTrainWorker pid=75474, ip=10.0.55.91)   warnings.warn(\n",
      "(RayTrainWorker pid=75473, ip=10.0.55.91) /home/ray/anaconda3/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "(RayTrainWorker pid=75473, ip=10.0.55.91)   warnings.warn(\n",
      "(RayTrainWorker pid=75471, ip=10.0.55.91) /home/ray/anaconda3/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "(RayTrainWorker pid=75471, ip=10.0.55.91)   warnings.warn(\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) /home/ray/anaconda3/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)   warnings.warn(\n",
      "(RayTrainWorker pid=66667, ip=10.0.15.54) Using cuda_amp half precision backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:10,728] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed info: version=0.8.0, git-hash=unknown, git-branch=unknown\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=75470, ip=10.0.55.91) Using cuda_amp half precision backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:11,281] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "(RayTrainWorker pid=66675, ip=10.0.15.54) Installed CUDA version 11.8 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination\n",
      "(RayTrainWorker pid=66679, ip=10.0.15.54) Installed CUDA version 11.8 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination\n",
      "(RayTrainWorker pid=66667, ip=10.0.15.54) Installed CUDA version 11.8 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination\n",
      "(RayTrainWorker pid=66669, ip=10.0.15.54) Installed CUDA version 11.8 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination\n",
      "(RayTrainWorker pid=66670, ip=10.0.15.54) Installed CUDA version 11.8 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination\n",
      "(RayTrainWorker pid=66677, ip=10.0.15.54) Installed CUDA version 11.8 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination\n",
      "(RayTrainWorker pid=66671, ip=10.0.15.54) Installed CUDA version 11.8 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination\n",
      "(RayTrainWorker pid=66668, ip=10.0.15.54) Installed CUDA version 11.8 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) Installed CUDA version 11.8 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination\n",
      "(RayTrainWorker pid=75473, ip=10.0.55.91) Installed CUDA version 11.8 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination\n",
      "(RayTrainWorker pid=75474, ip=10.0.55.91) Installed CUDA version 11.8 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination\n",
      "(RayTrainWorker pid=75476, ip=10.0.55.91) Installed CUDA version 11.8 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination\n",
      "(RayTrainWorker pid=75471, ip=10.0.55.91) Installed CUDA version 11.8 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination\n",
      "(RayTrainWorker pid=75472, ip=10.0.55.91) Installed CUDA version 11.8 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination\n",
      "(RayTrainWorker pid=75477, ip=10.0.55.91) Installed CUDA version 11.8 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination\n",
      "(RayTrainWorker pid=75479, ip=10.0.55.91) Installed CUDA version 11.8 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=66671, ip=10.0.15.54) Using /home/ray/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "(RayTrainWorker pid=66675, ip=10.0.15.54) Using /home/ray/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "(RayTrainWorker pid=66679, ip=10.0.15.54) Using /home/ray/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "(RayTrainWorker pid=66667, ip=10.0.15.54) Using /home/ray/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "(RayTrainWorker pid=66669, ip=10.0.15.54) Using /home/ray/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "(RayTrainWorker pid=66670, ip=10.0.15.54) Using /home/ray/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "(RayTrainWorker pid=66677, ip=10.0.15.54) Using /home/ray/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "(RayTrainWorker pid=66668, ip=10.0.15.54) Using /home/ray/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "(RayTrainWorker pid=66677, ip=10.0.15.54) Detected CUDA files, patching ldflags\n",
      "(RayTrainWorker pid=66677, ip=10.0.15.54) Emitting ninja build file /home/ray/.cache/torch_extensions/py38_cu117/cpu_adam/build.ninja...\n",
      "(RayTrainWorker pid=66677, ip=10.0.15.54) Building extension module cpu_adam...\n",
      "(RayTrainWorker pid=66677, ip=10.0.15.54) Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "(RayTrainWorker pid=66671, ip=10.0.15.54) Loading extension module cpu_adam...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=66679, ip=10.0.15.54) Time to load cpu_adam op: 2.8483667373657227 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=66679, ip=10.0.15.54) Loading extension module cpu_adam...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=66667, ip=10.0.15.54) Time to load cpu_adam op: 2.833568811416626 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=66667, ip=10.0.15.54) Loading extension module cpu_adam...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=66669, ip=10.0.15.54) Time to load cpu_adam op: 2.840237617492676 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=66669, ip=10.0.15.54) Loading extension module cpu_adam...\n",
      "(RayTrainWorker pid=66670, ip=10.0.15.54) Loading extension module cpu_adam...\n",
      "(RayTrainWorker pid=75472, ip=10.0.55.91) Using /home/ray/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=66670, ip=10.0.15.54) Time to load cpu_adam op: 2.844681978225708 seconds\n",
      "(RayTrainWorker pid=66677, ip=10.0.15.54) ninja: no work to do.\n",
      "(RayTrainWorker pid=66677, ip=10.0.15.54) Time to load cpu_adam op: 2.8449223041534424 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=75476, ip=10.0.55.91) Using /home/ray/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=66671, ip=10.0.15.54) Time to load cpu_adam op: 2.8324334621429443 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=75474, ip=10.0.55.91) Using /home/ray/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "(RayTrainWorker pid=66677, ip=10.0.15.54) Loading extension module cpu_adam...\n",
      "(RayTrainWorker pid=75473, ip=10.0.55.91) Using /home/ray/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "(RayTrainWorker pid=75471, ip=10.0.55.91) Using /home/ray/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "(RayTrainWorker pid=66668, ip=10.0.15.54) Loading extension module cpu_adam...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=66668, ip=10.0.15.54) Time to load cpu_adam op: 2.832155704498291 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=75477, ip=10.0.55.91) Using /home/ray/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "(RayTrainWorker pid=66675, ip=10.0.15.54) Loading extension module cpu_adam...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=66675, ip=10.0.15.54) Time to load cpu_adam op: 2.9075350761413574 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=75470, ip=10.0.55.91) Using /home/ray/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "(RayTrainWorker pid=75479, ip=10.0.55.91) Using /home/ray/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "(RayTrainWorker pid=75474, ip=10.0.55.91) Detected CUDA files, patching ldflags\n",
      "(RayTrainWorker pid=75474, ip=10.0.55.91) Emitting ninja build file /home/ray/.cache/torch_extensions/py38_cu117/cpu_adam/build.ninja...\n",
      "(RayTrainWorker pid=75474, ip=10.0.55.91) Building extension module cpu_adam...\n",
      "(RayTrainWorker pid=75474, ip=10.0.55.91) Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "(RayTrainWorker pid=75472, ip=10.0.55.91) Loading extension module cpu_adam...\n",
      "(RayTrainWorker pid=75476, ip=10.0.55.91) Loading extension module cpu_adam...\n",
      "(RayTrainWorker pid=75474, ip=10.0.55.91) Loading extension module cpu_adam...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=75473, ip=10.0.55.91) Time to load cpu_adam op: 2.924166679382324 seconds\n",
      "(RayTrainWorker pid=75474, ip=10.0.55.91) ninja: no work to do.\n",
      "(RayTrainWorker pid=75474, ip=10.0.55.91) Time to load cpu_adam op: 2.8887887001037598 seconds\n",
      "(RayTrainWorker pid=75476, ip=10.0.55.91) Time to load cpu_adam op: 2.9388110637664795 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=75470, ip=10.0.55.91) Loading extension module cpu_adam...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=75471, ip=10.0.55.91) Time to load cpu_adam op: 2.9350662231445312 seconds\n",
      "(RayTrainWorker pid=75472, ip=10.0.55.91) Time to load cpu_adam op: 2.9407477378845215 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=75473, ip=10.0.55.91) Loading extension module cpu_adam...\n",
      "(RayTrainWorker pid=75471, ip=10.0.55.91) Loading extension module cpu_adam...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=75470, ip=10.0.55.91) Time to load cpu_adam op: 2.923283100128174 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=75477, ip=10.0.55.91) Loading extension module cpu_adam...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=75477, ip=10.0.55.91) Time to load cpu_adam op: 2.933358907699585 seconds\n",
      "(RayTrainWorker pid=75479, ip=10.0.55.91) Time to load cpu_adam op: 2.929224729537964 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=75479, ip=10.0.55.91) Loading extension module cpu_adam...\n",
      "(RayTrainWorker pid=66671, ip=10.0.15.54) Using /home/ray/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "(RayTrainWorker pid=66675, ip=10.0.15.54) Using /home/ray/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "(RayTrainWorker pid=66679, ip=10.0.15.54) Using /home/ray/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "(RayTrainWorker pid=66667, ip=10.0.15.54) Using /home/ray/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "(RayTrainWorker pid=66669, ip=10.0.15.54) Using /home/ray/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "(RayTrainWorker pid=66670, ip=10.0.15.54) Using /home/ray/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "(RayTrainWorker pid=66677, ip=10.0.15.54) Using /home/ray/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "(RayTrainWorker pid=66668, ip=10.0.15.54) Using /home/ray/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=66667, ip=10.0.15.54) Adam Optimizer #0 is created with AVX2 arithmetic capability.\n",
      "(RayTrainWorker pid=66667, ip=10.0.15.54) Config: alpha=0.001000, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=75474, ip=10.0.55.91) Using /home/ray/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "(RayTrainWorker pid=66670, ip=10.0.15.54) Emitting ninja build file /home/ray/.cache/torch_extensions/py38_cu117/utils/build.ninja...\n",
      "(RayTrainWorker pid=66670, ip=10.0.15.54) Building extension module utils...\n",
      "(RayTrainWorker pid=66670, ip=10.0.15.54) Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=66667, ip=10.0.15.54) Time to load utils op: 0.3039577007293701 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=66667, ip=10.0.15.54) Loading extension module utils...\n",
      "(RayTrainWorker pid=66670, ip=10.0.15.54) Loading extension module utils...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=66670, ip=10.0.15.54) ninja: no work to do.\n",
      "(RayTrainWorker pid=66670, ip=10.0.15.54) Time to load utils op: 0.34416699409484863 seconds\n",
      "(RayTrainWorker pid=66677, ip=10.0.15.54) Time to load utils op: 0.3037292957305908 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=66677, ip=10.0.15.54) Loading extension module utils...\n",
      "(RayTrainWorker pid=75472, ip=10.0.55.91) Using /home/ray/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "(RayTrainWorker pid=75476, ip=10.0.55.91) Using /home/ray/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:18,715] [INFO] [logging.py:68:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=75474, ip=10.0.55.91) Emitting ninja build file /home/ray/.cache/torch_extensions/py38_cu117/utils/build.ninja...\n",
      "(RayTrainWorker pid=75474, ip=10.0.55.91) Building extension module utils...\n",
      "(RayTrainWorker pid=75474, ip=10.0.55.91) Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "(RayTrainWorker pid=66671, ip=10.0.15.54) Loading extension module utils...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=66668, ip=10.0.15.54) Time to load utils op: 0.40503859519958496 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=66675, ip=10.0.15.54) Loading extension module utils...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=66675, ip=10.0.15.54) Time to load utils op: 0.4038050174713135 seconds\n",
      "(RayTrainWorker pid=66679, ip=10.0.15.54) Time to load utils op: 0.40484142303466797 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=66679, ip=10.0.15.54) Loading extension module utils...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=66669, ip=10.0.15.54) Time to load utils op: 0.40423083305358887 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=66669, ip=10.0.15.54) Loading extension module utils...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=66671, ip=10.0.15.54) Time to load utils op: 0.4048638343811035 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=66668, ip=10.0.15.54) Loading extension module utils...\n",
      "(RayTrainWorker pid=75477, ip=10.0.55.91) Using /home/ray/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "(RayTrainWorker pid=75476, ip=10.0.55.91) Loading extension module utils...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:18,729] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:18,729] [INFO] [utils.py:52:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:18,729] [INFO] [logging.py:68:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) Adam Optimizer #0 is created with AVX2 arithmetic capability.\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) Config: alpha=0.001000, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=75474, ip=10.0.55.91) Loading extension module utils...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=75474, ip=10.0.55.91) ninja: no work to do.\n",
      "(RayTrainWorker pid=75474, ip=10.0.55.91) Time to load utils op: 0.3677384853363037 seconds\n",
      "(RayTrainWorker pid=75476, ip=10.0.55.91) Time to load utils op: 0.10255312919616699 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=75473, ip=10.0.55.91) Using /home/ray/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "(RayTrainWorker pid=75471, ip=10.0.55.91) Using /home/ray/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=75479, ip=10.0.55.91) Time to load utils op: 0.10249686241149902 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=75479, ip=10.0.55.91) Using /home/ray/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "(RayTrainWorker pid=75479, ip=10.0.55.91) Loading extension module utils...\n",
      "(RayTrainWorker pid=75472, ip=10.0.55.91) Loading extension module utils...\n",
      "(RayTrainWorker pid=75477, ip=10.0.55.91) Loading extension module utils...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=75473, ip=10.0.55.91) Time to load utils op: 0.10236072540283203 seconds\n",
      "(RayTrainWorker pid=75471, ip=10.0.55.91) Time to load utils op: 0.10248851776123047 seconds\n",
      "(RayTrainWorker pid=75472, ip=10.0.55.91) Time to load utils op: 0.20300722122192383 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=75473, ip=10.0.55.91) Loading extension module utils...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=75477, ip=10.0.55.91) Time to load utils op: 0.10242915153503418 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=75471, ip=10.0.55.91) Loading extension module utils...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:19,050] [INFO] [utils.py:831:see_memory_usage] Stage 3 initialize beginning\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:19,050] [INFO] [utils.py:832:see_memory_usage] MA 1.58 GB         Max_MA 1.97 GB         CA 13.57 GB         Max_CA 14 GB \n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:19,051] [INFO] [utils.py:840:see_memory_usage] CPU Virtual Memory:  used = 41.47 GB, percent = 5.5%\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:19,054] [INFO] [stage3.py:114:__init__] Reduce bucket size 16777216\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:19,054] [INFO] [stage3.py:115:__init__] Prefetch bucket size 15099494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=75470, ip=10.0.55.91) Using /home/ray/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) Emitting ninja build file /home/ray/.cache/torch_extensions/py38_cu117/utils/build.ninja...\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) Building extension module utils...\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=75470, ip=10.0.55.91) ninja: no work to do.\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) Time to load utils op: 0.3839387893676758 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=75470, ip=10.0.55.91) Loading extension module utils...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:19,671] [INFO] [utils.py:831:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:19,672] [INFO] [utils.py:832:see_memory_usage] MA 1.58 GB         Max_MA 1.58 GB         CA 13.57 GB         Max_CA 14 GB \n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:19,672] [INFO] [utils.py:840:see_memory_usage] CPU Virtual Memory:  used = 41.48 GB, percent = 5.5%\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) Parameter Offload: Total persistent parameters: 811008 in 114 params\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:19,906] [INFO] [utils.py:831:see_memory_usage] DeepSpeedZeRoOffload initialize [end]\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:19,907] [INFO] [utils.py:832:see_memory_usage] MA 1.58 GB         Max_MA 1.58 GB         CA 13.57 GB         Max_CA 14 GB \n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:19,907] [INFO] [utils.py:840:see_memory_usage] CPU Virtual Memory:  used = 41.48 GB, percent = 5.5%\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:20,127] [INFO] [utils.py:831:see_memory_usage] Before creating fp16 partitions\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:20,128] [INFO] [utils.py:832:see_memory_usage] MA 1.58 GB         Max_MA 1.58 GB         CA 13.57 GB         Max_CA 14 GB \n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:20,128] [INFO] [utils.py:840:see_memory_usage] CPU Virtual Memory:  used = 41.48 GB, percent = 5.5%\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:21,149] [INFO] [utils.py:831:see_memory_usage] After creating fp16 partitions: 1\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:21,150] [INFO] [utils.py:832:see_memory_usage] MA 1.58 GB         Max_MA 1.58 GB         CA 7.38 GB         Max_CA 14 GB \n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:21,151] [INFO] [utils.py:840:see_memory_usage] CPU Virtual Memory:  used = 44.23 GB, percent = 5.9%\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:21,404] [INFO] [utils.py:831:see_memory_usage] Before creating fp32 partitions\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:21,405] [INFO] [utils.py:832:see_memory_usage] MA 1.58 GB         Max_MA 1.58 GB         CA 7.38 GB         Max_CA 7 GB \n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:21,405] [INFO] [utils.py:840:see_memory_usage] CPU Virtual Memory:  used = 49.31 GB, percent = 6.6%\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:22,870] [INFO] [utils.py:831:see_memory_usage] After creating fp32 partitions\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:22,870] [INFO] [utils.py:832:see_memory_usage] MA 1.58 GB         Max_MA 1.58 GB         CA 7.38 GB         Max_CA 7 GB \n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:22,871] [INFO] [utils.py:840:see_memory_usage] CPU Virtual Memory:  used = 52.77 GB, percent = 7.1%\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:23,239] [INFO] [utils.py:831:see_memory_usage] Before initializing optimizer states\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:23,239] [INFO] [utils.py:832:see_memory_usage] MA 1.58 GB         Max_MA 1.58 GB         CA 7.38 GB         Max_CA 7 GB \n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:23,240] [INFO] [utils.py:840:see_memory_usage] CPU Virtual Memory:  used = 62.61 GB, percent = 8.4%\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:25,401] [INFO] [utils.py:831:see_memory_usage] After initializing optimizer states\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:25,402] [INFO] [utils.py:832:see_memory_usage] MA 1.58 GB         Max_MA 1.58 GB         CA 7.38 GB         Max_CA 7 GB \n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:25,402] [INFO] [utils.py:840:see_memory_usage] CPU Virtual Memory:  used = 86.65 GB, percent = 11.6%\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:25,402] [INFO] [stage3.py:382:_setup_for_real_optimizer] optimizer state initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=66671, ip=10.0.15.54) Using /home/ray/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "(RayTrainWorker pid=66671, ip=10.0.15.54) No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "(RayTrainWorker pid=66671, ip=10.0.15.54) Loading extension module utils...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=66668, ip=10.0.15.54) Time to load utils op: 0.0005533695220947266 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=66675, ip=10.0.15.54) Using /home/ray/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "(RayTrainWorker pid=66675, ip=10.0.15.54) No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "(RayTrainWorker pid=66675, ip=10.0.15.54) Loading extension module utils...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=66675, ip=10.0.15.54) Time to load utils op: 0.0007190704345703125 seconds\n",
      "(RayTrainWorker pid=66679, ip=10.0.15.54) Time to load utils op: 0.0005917549133300781 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=66679, ip=10.0.15.54) Using /home/ray/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "(RayTrainWorker pid=66679, ip=10.0.15.54) No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "(RayTrainWorker pid=66679, ip=10.0.15.54) Loading extension module utils...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=66667, ip=10.0.15.54) Time to load utils op: 0.0004563331604003906 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=66667, ip=10.0.15.54) Using /home/ray/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "(RayTrainWorker pid=66667, ip=10.0.15.54) No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "(RayTrainWorker pid=66667, ip=10.0.15.54) Loading extension module utils...\n",
      "(RayTrainWorker pid=66667, ip=10.0.15.54) ***** Running training *****\n",
      "(RayTrainWorker pid=66667, ip=10.0.15.54)   Num examples = 61\n",
      "(RayTrainWorker pid=66667, ip=10.0.15.54)   Num Epochs = 2\n",
      "(RayTrainWorker pid=66667, ip=10.0.15.54)   Instantaneous batch size per device = 6\n",
      "(RayTrainWorker pid=66667, ip=10.0.15.54)   Total train batch size (w. parallel, distributed & accumulation) = 96\n",
      "(RayTrainWorker pid=66667, ip=10.0.15.54)   Gradient Accumulation steps = 1\n",
      "(RayTrainWorker pid=66667, ip=10.0.15.54)   Total optimization steps = 22\n",
      "(RayTrainWorker pid=66667, ip=10.0.15.54)   Number of trainable parameters = 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=66669, ip=10.0.15.54) Time to load utils op: 0.0005140304565429688 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=66669, ip=10.0.15.54) Using /home/ray/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "(RayTrainWorker pid=66669, ip=10.0.15.54) No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "(RayTrainWorker pid=66669, ip=10.0.15.54) Loading extension module utils...\n",
      "(RayTrainWorker pid=66670, ip=10.0.15.54) Using /home/ray/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "(RayTrainWorker pid=66670, ip=10.0.15.54) No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "(RayTrainWorker pid=66670, ip=10.0.15.54) Loading extension module utils...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=66670, ip=10.0.15.54) Time to load utils op: 0.0004949569702148438 seconds\n",
      "(RayTrainWorker pid=66677, ip=10.0.15.54) Time to load utils op: 0.0006148815155029297 seconds\n",
      "(RayTrainWorker pid=66671, ip=10.0.15.54) Time to load utils op: 0.0005185604095458984 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=66677, ip=10.0.15.54) Using /home/ray/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "(RayTrainWorker pid=66677, ip=10.0.15.54) No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "(RayTrainWorker pid=66677, ip=10.0.15.54) Loading extension module utils...\n",
      "(RayTrainWorker pid=66668, ip=10.0.15.54) Using /home/ray/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "(RayTrainWorker pid=66668, ip=10.0.15.54) No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "(RayTrainWorker pid=66668, ip=10.0.15.54) Loading extension module utils...\n",
      "(RayTrainWorker pid=75472, ip=10.0.55.91) Using /home/ray/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "(RayTrainWorker pid=75472, ip=10.0.55.91) No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "(RayTrainWorker pid=75472, ip=10.0.55.91) Loading extension module utils...\n",
      "(RayTrainWorker pid=75477, ip=10.0.55.91) Using /home/ray/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "(RayTrainWorker pid=75477, ip=10.0.55.91) No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "(RayTrainWorker pid=75477, ip=10.0.55.91) Loading extension module utils...\n",
      "(RayTrainWorker pid=75476, ip=10.0.55.91) Using /home/ray/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "(RayTrainWorker pid=75476, ip=10.0.55.91) No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "(RayTrainWorker pid=75476, ip=10.0.55.91) Loading extension module utils...\n",
      "(RayTrainWorker pid=75474, ip=10.0.55.91) Using /home/ray/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "(RayTrainWorker pid=75474, ip=10.0.55.91) No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "(RayTrainWorker pid=75474, ip=10.0.55.91) Loading extension module utils...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=75473, ip=10.0.55.91) Time to load utils op: 0.0005252361297607422 seconds\n",
      "(RayTrainWorker pid=75474, ip=10.0.55.91) Time to load utils op: 0.0010602474212646484 seconds\n",
      "(RayTrainWorker pid=75476, ip=10.0.55.91) Time to load utils op: 0.0011684894561767578 seconds\n",
      "(RayTrainWorker pid=75471, ip=10.0.55.91) Time to load utils op: 0.0017752647399902344 seconds\n",
      "(RayTrainWorker pid=75472, ip=10.0.55.91) Time to load utils op: 0.001064300537109375 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=75473, ip=10.0.55.91) Using /home/ray/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "(RayTrainWorker pid=75473, ip=10.0.55.91) No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "(RayTrainWorker pid=75473, ip=10.0.55.91) Loading extension module utils...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=75477, ip=10.0.55.91) Time to load utils op: 0.0005650520324707031 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=75471, ip=10.0.55.91) Using /home/ray/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "(RayTrainWorker pid=75471, ip=10.0.55.91) No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "(RayTrainWorker pid=75471, ip=10.0.55.91) Loading extension module utils...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=75479, ip=10.0.55.91) Time to load utils op: 0.0009174346923828125 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=75479, ip=10.0.55.91) Using /home/ray/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "(RayTrainWorker pid=75479, ip=10.0.55.91) No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "(RayTrainWorker pid=75479, ip=10.0.55.91) Loading extension module utils...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:30,034] [INFO] [utils.py:831:see_memory_usage] After initializing ZeRO optimizer\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:30,034] [INFO] [utils.py:832:see_memory_usage] MA 0.85 GB         Max_MA 1.62 GB         CA 7.38 GB         Max_CA 7 GB \n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:30,035] [INFO] [utils.py:840:see_memory_usage] CPU Virtual Memory:  used = 92.28 GB, percent = 12.3%\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:30,035] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:30,035] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed using client callable to create LR scheduler\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:30,035] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed LR Scheduler = <torch.optim.lr_scheduler.LambdaLR object at 0x7f83ec841a30>\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:30,035] [INFO] [logging.py:68:log_dist] [Rank 0] step=0, skipped=0, lr=[0.001], mom=[[0.9, 0.999]]\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:30,036] [INFO] [config.py:1008:print] DeepSpeedEngine configuration:\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:30,037] [INFO] [config.py:1012:print]   activation_checkpointing_config  {\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)     \"partition_activations\": false, \n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)     \"contiguous_memory_optimization\": false, \n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)     \"cpu_checkpointing\": false, \n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)     \"number_checkpoints\": null, \n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)     \"synchronize_checkpoint_boundary\": false, \n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)     \"profile\": false\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) }\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:30,037] [INFO] [config.py:1012:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:30,037] [INFO] [config.py:1012:print]   amp_enabled .................. False\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:30,037] [INFO] [config.py:1012:print]   amp_params ................... False\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:30,038] [INFO] [config.py:1012:print]   autotuning_config ............ {\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)     \"enabled\": false, \n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)     \"start_step\": null, \n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)     \"end_step\": null, \n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)     \"metric_path\": null, \n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)     \"arg_mappings\": null, \n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)     \"metric\": \"throughput\", \n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)     \"model_info\": null, \n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)     \"results_dir\": \"autotuning_results\", \n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)     \"exps_dir\": \"autotuning_exps\", \n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)     \"overwrite\": true, \n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)     \"fast\": true, \n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)     \"start_profile_step\": 3, \n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)     \"end_profile_step\": 5, \n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)     \"tuner_type\": \"gridsearch\", \n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)     \"tuner_early_stopping\": 5, \n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)     \"tuner_num_trials\": 50, \n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)     \"model_info_path\": null, \n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)     \"mp_size\": 1, \n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)     \"max_train_batch_size\": null, \n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)     \"min_train_batch_size\": 1, \n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)     \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)     \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)     \"num_tuning_micro_batch_sizes\": 3\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) }\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:30,038] [INFO] [config.py:1012:print]   bfloat16_enabled ............. True\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:30,038] [INFO] [config.py:1012:print]   checkpoint_parallel_write_pipeline  False\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:30,038] [INFO] [config.py:1012:print]   checkpoint_tag_validation_enabled  True\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:30,038] [INFO] [config.py:1012:print]   checkpoint_tag_validation_fail  False\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:30,038] [INFO] [config.py:1012:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f8413ef89d0>\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:30,038] [INFO] [config.py:1012:print]   communication_data_type ...... None\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:30,038] [INFO] [config.py:1012:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:30,038] [INFO] [config.py:1012:print]   curriculum_enabled_legacy .... False\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:30,038] [INFO] [config.py:1012:print]   curriculum_params_legacy ..... False\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:30,038] [INFO] [config.py:1012:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:30,038] [INFO] [config.py:1012:print]   data_efficiency_enabled ...... False\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:30,038] [INFO] [config.py:1012:print]   dataloader_drop_last ......... False\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:30,038] [INFO] [config.py:1012:print]   disable_allgather ............ False\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:30,038] [INFO] [config.py:1012:print]   dump_state ................... False\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:30,038] [INFO] [config.py:1012:print]   dynamic_loss_scale_args ...... None\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:30,038] [INFO] [config.py:1012:print]   eigenvalue_enabled ........... False\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:30,038] [INFO] [config.py:1012:print]   eigenvalue_gas_boundary_resolution  1\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:30,038] [INFO] [config.py:1012:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:30,038] [INFO] [config.py:1012:print]   eigenvalue_layer_num ......... 0\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:30,038] [INFO] [config.py:1012:print]   eigenvalue_max_iter .......... 100\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:30,038] [INFO] [config.py:1012:print]   eigenvalue_stability ......... 1e-06\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:30,038] [INFO] [config.py:1012:print]   eigenvalue_tol ............... 0.01\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:30,038] [INFO] [config.py:1012:print]   eigenvalue_verbose ........... False\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:30,038] [INFO] [config.py:1012:print]   elasticity_enabled ........... False\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:30,038] [INFO] [config.py:1012:print]   flops_profiler_config ........ {\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)     \"enabled\": false, \n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)     \"profile_step\": 1, \n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)     \"module_depth\": -1, \n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)     \"top_modules\": 1, \n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)     \"detailed\": true, \n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)     \"output_file\": null\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) }\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:30,038] [INFO] [config.py:1012:print]   fp16_auto_cast ............... None\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:30,038] [INFO] [config.py:1012:print]   fp16_enabled ................. False\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:30,038] [INFO] [config.py:1012:print]   fp16_master_weights_and_gradients  False\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:30,038] [INFO] [config.py:1012:print]   global_rank .................. 0\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:30,038] [INFO] [config.py:1012:print]   grad_accum_dtype ............. None\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:30,038] [INFO] [config.py:1012:print]   gradient_accumulation_steps .. 1\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:30,039] [INFO] [config.py:1012:print]   gradient_clipping ............ 1.0\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:30,039] [INFO] [config.py:1012:print]   gradient_predivide_factor .... 1.0\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:30,039] [INFO] [config.py:1012:print]   initial_dynamic_scale ........ 1\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:30,039] [INFO] [config.py:1012:print]   load_universal_checkpoint .... False\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:30,039] [INFO] [config.py:1012:print]   loss_scale ................... 1.0\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:30,039] [INFO] [config.py:1012:print]   memory_breakdown ............. False\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:30,039] [INFO] [config.py:1012:print]   monitor_config ............... <deepspeed.monitor.config.DeepSpeedMonitorConfig object at 0x7f8413ef8b50>\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:30,039] [INFO] [config.py:1012:print]   nebula_config ................ {\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)     \"enabled\": false, \n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)     \"persistent_storage_path\": null, \n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)     \"persistent_time_interval\": 100, \n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)     \"num_of_version_in_retention\": 2, \n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)     \"enable_nebula_load\": true, \n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)     \"load_path\": null\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) }\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:30,039] [INFO] [config.py:1012:print]   optimizer_legacy_fusion ...... False\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:30,039] [INFO] [config.py:1012:print]   optimizer_name ............... adamw\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:30,039] [INFO] [config.py:1012:print]   optimizer_params ............. {'lr': 0.001, 'betas': [0.9, 0.999], 'eps': 1e-08}\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:30,039] [INFO] [config.py:1012:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:30,039] [INFO] [config.py:1012:print]   pld_enabled .................. False\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:30,039] [INFO] [config.py:1012:print]   pld_params ................... False\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:30,039] [INFO] [config.py:1012:print]   prescale_gradients ........... False\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:30,039] [INFO] [config.py:1012:print]   scheduler_name ............... None\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:30,039] [INFO] [config.py:1012:print]   scheduler_params ............. None\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:30,039] [INFO] [config.py:1012:print]   sparse_attention ............. None\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:30,039] [INFO] [config.py:1012:print]   sparse_gradients_enabled ..... False\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:30,039] [INFO] [config.py:1012:print]   steps_per_print .............. 1\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:30,039] [INFO] [config.py:1012:print]   train_batch_size ............. 96\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:30,039] [INFO] [config.py:1012:print]   train_micro_batch_size_per_gpu  6\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:30,039] [INFO] [config.py:1012:print]   use_node_local_storage ....... False\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:30,039] [INFO] [config.py:1012:print]   wall_clock_breakdown ......... False\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:30,039] [INFO] [config.py:1012:print]   world_size ................... 16\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:30,039] [INFO] [config.py:1012:print]   zero_allow_untested_optimizer  False\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:30,039] [INFO] [config.py:1012:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=16777216 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=15099494 param_persistence_threshold=40960 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=True stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=True\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:30,039] [INFO] [config.py:1012:print]   zero_enabled ................. True\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:30,039] [INFO] [config.py:1012:print]   zero_optimization_stage ...... 3\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:56:30,040] [INFO] [config.py:997:print_user_config]   json = {\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)     \"fp16\": {\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)         \"enabled\": false, \n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)         \"initial_scale_power\": 32\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)     }, \n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)     \"bf16\": {\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)         \"enabled\": true\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)     }, \n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)     \"optimizer\": {\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)         \"type\": \"AdamW\", \n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)         \"params\": {\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)             \"lr\": 0.001, \n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)             \"betas\": [0.9, 0.999], \n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)             \"eps\": 1e-08\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)         }\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)     }, \n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)     \"zero_optimization\": {\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)         \"stage\": 3, \n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)         \"offload_optimizer\": {\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)             \"device\": \"cpu\", \n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)             \"pin_memory\": false\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)         }, \n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)         \"overlap_comm\": true, \n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)         \"contiguous_gradients\": true, \n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)         \"reduce_bucket_size\": 1.677722e+07, \n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)         \"stage3_prefetch_bucket_size\": 1.509949e+07, \n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)         \"stage3_param_persistence_threshold\": 4.096000e+04, \n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)         \"gather_16bit_weights_on_model_save\": true, \n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)         \"round_robin_gradients\": true\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)     }, \n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)     \"gradient_accumulation_steps\": 1, \n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)     \"gradient_clipping\": 1.0, \n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)     \"steps_per_print\": 1, \n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)     \"train_batch_size\": 96, \n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)     \"train_micro_batch_size_per_gpu\": 6, \n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)     \"wall_clock_breakdown\": false\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) }\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) Time to load utils op: 0.0012660026550292969 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=75470, ip=10.0.55.91) Using /home/ray/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) Loading extension module utils...\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) ***** Running training *****\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)   Num examples = 61\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)   Num Epochs = 2\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)   Instantaneous batch size per device = 6\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)   Total train batch size (w. parallel, distributed & accumulation) = 96\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)   Gradient Accumulation steps = 1\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)   Total optimization steps = 22\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)   Number of trainable parameters = 0\n",
      "(RayTrainWorker pid=66671, ip=10.0.15.54) /home/ray/anaconda3/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "(RayTrainWorker pid=66671, ip=10.0.15.54)   warnings.warn(\n",
      "(RayTrainWorker pid=66675, ip=10.0.15.54) /home/ray/anaconda3/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "(RayTrainWorker pid=66675, ip=10.0.15.54)   warnings.warn(\n",
      "(RayTrainWorker pid=66679, ip=10.0.15.54) /home/ray/anaconda3/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "(RayTrainWorker pid=66679, ip=10.0.15.54)   warnings.warn(\n",
      "(RayTrainWorker pid=66667, ip=10.0.15.54) /home/ray/anaconda3/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "(RayTrainWorker pid=66667, ip=10.0.15.54)   warnings.warn(\n",
      "(RayTrainWorker pid=66669, ip=10.0.15.54) /home/ray/anaconda3/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "(RayTrainWorker pid=66669, ip=10.0.15.54)   warnings.warn(\n",
      "(RayTrainWorker pid=66670, ip=10.0.15.54) /home/ray/anaconda3/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "(RayTrainWorker pid=66670, ip=10.0.15.54)   warnings.warn(\n",
      "(RayTrainWorker pid=66677, ip=10.0.15.54) /home/ray/anaconda3/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "(RayTrainWorker pid=66677, ip=10.0.15.54)   warnings.warn(\n",
      "(RayTrainWorker pid=66668, ip=10.0.15.54) /home/ray/anaconda3/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "(RayTrainWorker pid=66668, ip=10.0.15.54)   warnings.warn(\n",
      "(RayTrainWorker pid=75472, ip=10.0.55.91) /home/ray/anaconda3/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "(RayTrainWorker pid=75472, ip=10.0.55.91)   warnings.warn(\n",
      "(RayTrainWorker pid=75477, ip=10.0.55.91) /home/ray/anaconda3/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "(RayTrainWorker pid=75477, ip=10.0.55.91)   warnings.warn(\n",
      "(RayTrainWorker pid=75476, ip=10.0.55.91) /home/ray/anaconda3/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "(RayTrainWorker pid=75476, ip=10.0.55.91)   warnings.warn(\n",
      "(RayTrainWorker pid=75474, ip=10.0.55.91) /home/ray/anaconda3/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "(RayTrainWorker pid=75474, ip=10.0.55.91)   warnings.warn(\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) /home/ray/anaconda3/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)   warnings.warn(\n",
      "(RayTrainWorker pid=75473, ip=10.0.55.91) /home/ray/anaconda3/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "(RayTrainWorker pid=75473, ip=10.0.55.91)   warnings.warn(\n",
      "(RayTrainWorker pid=75471, ip=10.0.55.91) /home/ray/anaconda3/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "(RayTrainWorker pid=75471, ip=10.0.55.91)   warnings.warn(\n",
      "(RayTrainWorker pid=75479, ip=10.0.55.91) /home/ray/anaconda3/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "(RayTrainWorker pid=75479, ip=10.0.55.91)   warnings.warn(\n",
      "(RayTrainWorker pid=66671, ip=10.0.15.54) /home/ray/anaconda3/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.\n",
      "(RayTrainWorker pid=66671, ip=10.0.15.54)   warnings.warn(\n",
      "(RayTrainWorker pid=66675, ip=10.0.15.54) /home/ray/anaconda3/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.\n",
      "(RayTrainWorker pid=66675, ip=10.0.15.54)   warnings.warn(\n",
      "(RayTrainWorker pid=66679, ip=10.0.15.54) /home/ray/anaconda3/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.\n",
      "(RayTrainWorker pid=66679, ip=10.0.15.54)   warnings.warn(\n",
      "(RayTrainWorker pid=66667, ip=10.0.15.54) /home/ray/anaconda3/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.\n",
      "(RayTrainWorker pid=66667, ip=10.0.15.54)   warnings.warn(\n",
      "(RayTrainWorker pid=66669, ip=10.0.15.54) /home/ray/anaconda3/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.\n",
      "(RayTrainWorker pid=66669, ip=10.0.15.54)   warnings.warn(\n",
      "(RayTrainWorker pid=66670, ip=10.0.15.54) /home/ray/anaconda3/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.\n",
      "(RayTrainWorker pid=66670, ip=10.0.15.54)   warnings.warn(\n",
      "(RayTrainWorker pid=66677, ip=10.0.15.54) /home/ray/anaconda3/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.\n",
      "(RayTrainWorker pid=66677, ip=10.0.15.54)   warnings.warn(\n",
      "(RayTrainWorker pid=66668, ip=10.0.15.54) /home/ray/anaconda3/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.\n",
      "(RayTrainWorker pid=66668, ip=10.0.15.54)   warnings.warn(\n",
      "(RayTrainWorker pid=75472, ip=10.0.55.91) /home/ray/anaconda3/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.\n",
      "(RayTrainWorker pid=75472, ip=10.0.55.91)   warnings.warn(\n",
      "(RayTrainWorker pid=75477, ip=10.0.55.91) /home/ray/anaconda3/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.\n",
      "(RayTrainWorker pid=75477, ip=10.0.55.91)   warnings.warn(\n",
      "(RayTrainWorker pid=75476, ip=10.0.55.91) /home/ray/anaconda3/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.\n",
      "(RayTrainWorker pid=75476, ip=10.0.55.91)   warnings.warn(\n",
      "(RayTrainWorker pid=75474, ip=10.0.55.91) /home/ray/anaconda3/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.\n",
      "(RayTrainWorker pid=75474, ip=10.0.55.91)   warnings.warn(\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) /home/ray/anaconda3/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91)   warnings.warn(\n",
      "(RayTrainWorker pid=75473, ip=10.0.55.91) /home/ray/anaconda3/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.\n",
      "(RayTrainWorker pid=75473, ip=10.0.55.91)   warnings.warn(\n",
      "(RayTrainWorker pid=75471, ip=10.0.55.91) /home/ray/anaconda3/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.\n",
      "(RayTrainWorker pid=75471, ip=10.0.55.91)   warnings.warn(\n",
      "(RayTrainWorker pid=75479, ip=10.0.55.91) /home/ray/anaconda3/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.\n",
      "(RayTrainWorker pid=75479, ip=10.0.55.91)   warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                           </th><th style=\"text-align: right;\">  _time_this_iter_s</th><th style=\"text-align: right;\">  _timestamp</th><th style=\"text-align: right;\">  _training_iteration</th><th>date               </th><th>done  </th><th>episodes_total  </th><th style=\"text-align: right;\">  epoch</th><th>experiment_id                   </th><th style=\"text-align: right;\">  experiment_tag</th><th>hostname     </th><th style=\"text-align: right;\">  iterations_since_restore</th><th style=\"text-align: right;\">  learning_rate</th><th style=\"text-align: right;\">  loss</th><th>node_ip   </th><th style=\"text-align: right;\">  pid</th><th style=\"text-align: right;\">  step</th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  timesteps_since_restore</th><th>timesteps_total  </th><th style=\"text-align: right;\">  training_iteration</th><th>trial_id   </th><th style=\"text-align: right;\">  warmup_time</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>HuggingFaceTrainerPatched_82a20_00000</td><td style=\"text-align: right;\">            34.0216</td><td style=\"text-align: right;\">  1675991305</td><td style=\"text-align: right;\">                   21</td><td>2023-02-09_17-08-25</td><td>False </td><td>                </td><td style=\"text-align: right;\">1.90909</td><td>7019a843d6944159bd2fed18c10ea6f3</td><td style=\"text-align: right;\">               0</td><td>ip-10-0-55-91</td><td style=\"text-align: right;\">                        21</td><td style=\"text-align: right;\">    4.54545e-05</td><td style=\"text-align: right;\">0.9622</td><td>10.0.55.91</td><td style=\"text-align: right;\">74513</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">             807.908</td><td style=\"text-align: right;\">           34.0214</td><td style=\"text-align: right;\">       807.908</td><td style=\"text-align: right;\"> 1675991305</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  21</td><td>82a20_00000</td><td style=\"text-align: right;\">     0.173635</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=66667, ip=10.0.15.54) {'loss': 10.3281, 'learning_rate': 0.0009545454545454546, 'epoch': 0.09}\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:57:13,309] [INFO] [logging.py:68:log_dist] [Rank 0] step=1, skipped=0, lr=[0.0009545454545454546], mom=[[0.9, 0.999]]\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) {'loss': 10.3281, 'learning_rate': 0.0009545454545454546, 'epoch': 0.09}\n",
      "(RayTrainWorker pid=66667, ip=10.0.15.54) {'loss': 7.8867, 'learning_rate': 0.0009090909090909091, 'epoch': 0.18}\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:57:46,744] [WARNING] [stage3.py:1939:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding torch.cuda.empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:57:46,746] [INFO] [logging.py:68:log_dist] [Rank 0] step=2, skipped=0, lr=[0.0009090909090909091], mom=[[0.9, 0.999]]\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) {'loss': 7.8867, 'learning_rate': 0.0009090909090909091, 'epoch': 0.18}\n",
      "(RayTrainWorker pid=66667, ip=10.0.15.54) {'loss': 3.6348, 'learning_rate': 0.0008636363636363636, 'epoch': 0.27}\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:58:19,549] [WARNING] [stage3.py:1939:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding torch.cuda.empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:58:19,550] [INFO] [logging.py:68:log_dist] [Rank 0] step=3, skipped=0, lr=[0.0008636363636363636], mom=[[0.9, 0.999]]\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:58:19,551] [INFO] [timer.py:196:stop] epoch=0/micro_step=3/global_step=3, RunningAvgSamplesPerSec=2.9274366851671836, CurrSamplesPerSec=2.9274366851671836, MemAllocated=0.85GB, MaxMemAllocated=12.87GB\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) {'loss': 3.6348, 'learning_rate': 0.0008636363636363636, 'epoch': 0.27}\n",
      "(RayTrainWorker pid=66667, ip=10.0.15.54) {'loss': 5.4219, 'learning_rate': 0.0008181818181818183, 'epoch': 0.36}\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:58:52,799] [WARNING] [stage3.py:1939:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding torch.cuda.empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:58:52,800] [INFO] [logging.py:68:log_dist] [Rank 0] step=4, skipped=0, lr=[0.0008181818181818183], mom=[[0.9, 0.999]]\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:58:52,800] [INFO] [timer.py:196:stop] epoch=0/micro_step=4/global_step=4, RunningAvgSamplesPerSec=2.907800222727142, CurrSamplesPerSec=2.888425437305516, MemAllocated=0.85GB, MaxMemAllocated=12.87GB\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) {'loss': 5.4219, 'learning_rate': 0.0008181818181818183, 'epoch': 0.36}\n",
      "(RayTrainWorker pid=66667, ip=10.0.15.54) {'loss': 3.4922, 'learning_rate': 0.0007727272727272727, 'epoch': 0.45}\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:59:27,495] [WARNING] [stage3.py:1939:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding torch.cuda.empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:59:27,495] [INFO] [logging.py:68:log_dist] [Rank 0] step=5, skipped=0, lr=[0.0007727272727272727], mom=[[0.9, 0.999]]\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 16:59:27,496] [INFO] [timer.py:196:stop] epoch=0/micro_step=5/global_step=5, RunningAvgSamplesPerSec=2.8595791486520565, CurrSamplesPerSec=2.7677809773144664, MemAllocated=0.85GB, MaxMemAllocated=12.87GB\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) {'loss': 3.4922, 'learning_rate': 0.0007727272727272727, 'epoch': 0.45}\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 17:00:01,214] [WARNING] [stage3.py:1939:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding torch.cuda.empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 17:00:01,215] [INFO] [logging.py:68:log_dist] [Rank 0] step=6, skipped=0, lr=[0.0007272727272727273], mom=[[0.9, 0.999]]\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 17:00:01,215] [INFO] [timer.py:196:stop] epoch=0/micro_step=6/global_step=6, RunningAvgSamplesPerSec=2.856688198776307, CurrSamplesPerSec=2.8480503150775305, MemAllocated=0.85GB, MaxMemAllocated=12.87GB\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) {'loss': 5.5566, 'learning_rate': 0.0007272727272727273, 'epoch': 0.55}\n",
      "(RayTrainWorker pid=66667, ip=10.0.15.54) {'loss': 5.5566, 'learning_rate': 0.0007272727272727273, 'epoch': 0.55}\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 17:00:35,120] [WARNING] [stage3.py:1939:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding torch.cuda.empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 17:00:35,121] [INFO] [logging.py:68:log_dist] [Rank 0] step=7, skipped=0, lr=[0.0006818181818181818], mom=[[0.9, 0.999]]\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 17:00:35,122] [INFO] [timer.py:196:stop] epoch=0/micro_step=7/global_step=7, RunningAvgSamplesPerSec=2.8517836043285643, CurrSamplesPerSec=2.832332489978096, MemAllocated=0.85GB, MaxMemAllocated=12.87GB\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) {'loss': 2.0537, 'learning_rate': 0.0006818181818181818, 'epoch': 0.64}\n",
      "(RayTrainWorker pid=66667, ip=10.0.15.54) {'loss': 2.0537, 'learning_rate': 0.0006818181818181818, 'epoch': 0.64}\n",
      "(RayTrainWorker pid=66667, ip=10.0.15.54) {'loss': 2.8525, 'learning_rate': 0.0006363636363636364, 'epoch': 0.73}\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 17:01:08,742] [WARNING] [stage3.py:1939:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding torch.cuda.empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 17:01:08,743] [INFO] [logging.py:68:log_dist] [Rank 0] step=8, skipped=0, lr=[0.0006363636363636364], mom=[[0.9, 0.999]]\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 17:01:08,744] [INFO] [timer.py:196:stop] epoch=0/micro_step=8/global_step=8, RunningAvgSamplesPerSec=2.852543093433566, CurrSamplesPerSec=2.8563466150804544, MemAllocated=0.85GB, MaxMemAllocated=12.87GB\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) {'loss': 2.8525, 'learning_rate': 0.0006363636363636364, 'epoch': 0.73}\n",
      "(RayTrainWorker pid=66667, ip=10.0.15.54) {'loss': 2.5312, 'learning_rate': 0.0005909090909090909, 'epoch': 0.82}\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 17:01:42,690] [WARNING] [stage3.py:1939:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding torch.cuda.empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 17:01:42,690] [INFO] [logging.py:68:log_dist] [Rank 0] step=9, skipped=0, lr=[0.0005909090909090909], mom=[[0.9, 0.999]]\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 17:01:42,691] [INFO] [timer.py:196:stop] epoch=0/micro_step=9/global_step=9, RunningAvgSamplesPerSec=2.8491694100258296, CurrSamplesPerSec=2.8290937101407634, MemAllocated=0.85GB, MaxMemAllocated=12.87GB\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) {'loss': 2.5312, 'learning_rate': 0.0005909090909090909, 'epoch': 0.82}\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 17:02:15,964] [WARNING] [stage3.py:1939:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding torch.cuda.empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 17:02:15,965] [INFO] [logging.py:68:log_dist] [Rank 0] step=10, skipped=0, lr=[0.0005454545454545455], mom=[[0.9, 0.999]]\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 17:02:15,965] [INFO] [timer.py:196:stop] epoch=0/micro_step=10/global_step=10, RunningAvgSamplesPerSec=2.8537212288591656, CurrSamplesPerSec=2.885995796248688, MemAllocated=0.85GB, MaxMemAllocated=12.87GB\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) {'loss': 1.6611, 'learning_rate': 0.0005454545454545455, 'epoch': 0.91}\n",
      "(RayTrainWorker pid=66667, ip=10.0.15.54) {'loss': 1.6611, 'learning_rate': 0.0005454545454545455, 'epoch': 0.91}\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 17:02:49,173] [WARNING] [stage3.py:1939:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding torch.cuda.empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 17:02:49,174] [INFO] [logging.py:68:log_dist] [Rank 0] step=11, skipped=0, lr=[0.0005], mom=[[0.9, 0.999]]\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 17:02:49,174] [INFO] [timer.py:196:stop] epoch=0/micro_step=11/global_step=11, RunningAvgSamplesPerSec=2.85786886801671, CurrSamplesPerSec=2.891489120417473, MemAllocated=0.85GB, MaxMemAllocated=12.87GB\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) {'loss': 2.2119, 'learning_rate': 0.0005, 'epoch': 1.0}\n",
      "(RayTrainWorker pid=66667, ip=10.0.15.54) {'loss': 2.2119, 'learning_rate': 0.0005, 'epoch': 1.0}\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 17:03:22,709] [WARNING] [stage3.py:1939:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding torch.cuda.empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 17:03:22,710] [INFO] [logging.py:68:log_dist] [Rank 0] step=12, skipped=0, lr=[0.00045454545454545455], mom=[[0.9, 0.999]]\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 17:03:22,711] [INFO] [timer.py:196:stop] epoch=0/micro_step=12/global_step=12, RunningAvgSamplesPerSec=2.858476358703698, CurrSamplesPerSec=2.8639554191255545, MemAllocated=0.85GB, MaxMemAllocated=12.87GB\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) {'loss': 1.5195, 'learning_rate': 0.00045454545454545455, 'epoch': 1.09}\n",
      "(RayTrainWorker pid=66667, ip=10.0.15.54) {'loss': 1.5195, 'learning_rate': 0.00045454545454545455, 'epoch': 1.09}\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 17:03:56,432] [WARNING] [stage3.py:1939:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding torch.cuda.empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 17:03:56,433] [INFO] [logging.py:68:log_dist] [Rank 0] step=13, skipped=0, lr=[0.00040909090909090913], mom=[[0.9, 0.999]]\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 17:03:56,433] [INFO] [timer.py:196:stop] epoch=0/micro_step=13/global_step=13, RunningAvgSamplesPerSec=2.857497809479753, CurrSamplesPerSec=2.847749040332429, MemAllocated=0.85GB, MaxMemAllocated=12.87GB\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) {'loss': 1.4102, 'learning_rate': 0.00040909090909090913, 'epoch': 1.18}\n",
      "(RayTrainWorker pid=66667, ip=10.0.15.54) {'loss': 1.4102, 'learning_rate': 0.00040909090909090913, 'epoch': 1.18}\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 17:04:29,331] [WARNING] [stage3.py:1939:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding torch.cuda.empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 17:04:29,332] [INFO] [logging.py:68:log_dist] [Rank 0] step=14, skipped=0, lr=[0.00036363636363636367], mom=[[0.9, 0.999]]\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 17:04:29,332] [INFO] [timer.py:196:stop] epoch=0/micro_step=14/global_step=14, RunningAvgSamplesPerSec=2.8625388249937647, CurrSamplesPerSec=2.9191871072491002, MemAllocated=0.85GB, MaxMemAllocated=12.87GB\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) {'loss': 1.3804, 'learning_rate': 0.00036363636363636367, 'epoch': 1.27}\n",
      "(RayTrainWorker pid=66667, ip=10.0.15.54) {'loss': 1.3804, 'learning_rate': 0.00036363636363636367, 'epoch': 1.27}\n",
      "(RayTrainWorker pid=66667, ip=10.0.15.54) {'loss': 1.2314, 'learning_rate': 0.0003181818181818182, 'epoch': 1.36}\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 17:05:02,609] [WARNING] [stage3.py:1939:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding torch.cuda.empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 17:05:02,610] [INFO] [logging.py:68:log_dist] [Rank 0] step=15, skipped=0, lr=[0.0003181818181818182], mom=[[0.9, 0.999]]\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 17:05:02,610] [INFO] [timer.py:196:stop] epoch=0/micro_step=15/global_step=15, RunningAvgSamplesPerSec=2.864302980206461, CurrSamplesPerSec=2.8856437146203278, MemAllocated=0.85GB, MaxMemAllocated=12.87GB\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) {'loss': 1.2314, 'learning_rate': 0.0003181818181818182, 'epoch': 1.36}\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 17:05:36,011] [WARNING] [stage3.py:1939:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding torch.cuda.empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 17:05:36,012] [INFO] [logging.py:68:log_dist] [Rank 0] step=16, skipped=0, lr=[0.00027272727272727274], mom=[[0.9, 0.999]]\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 17:05:36,012] [INFO] [timer.py:196:stop] epoch=0/micro_step=16/global_step=16, RunningAvgSamplesPerSec=2.86506695118466, CurrSamplesPerSec=2.875035788610904, MemAllocated=0.85GB, MaxMemAllocated=12.87GB\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) {'loss': 1.1519, 'learning_rate': 0.00027272727272727274, 'epoch': 1.45}\n",
      "(RayTrainWorker pid=66667, ip=10.0.15.54) {'loss': 1.1519, 'learning_rate': 0.00027272727272727274, 'epoch': 1.45}\n",
      "(RayTrainWorker pid=66667, ip=10.0.15.54) {'loss': 1.0742, 'learning_rate': 0.00022727272727272727, 'epoch': 1.55}\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 17:06:10,064] [WARNING] [stage3.py:1939:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding torch.cuda.empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 17:06:10,065] [INFO] [logging.py:68:log_dist] [Rank 0] step=17, skipped=0, lr=[0.00022727272727272727], mom=[[0.9, 0.999]]\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 17:06:10,066] [INFO] [timer.py:196:stop] epoch=0/micro_step=17/global_step=17, RunningAvgSamplesPerSec=2.862004921878851, CurrSamplesPerSec=2.8198136122493165, MemAllocated=0.85GB, MaxMemAllocated=12.87GB\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) {'loss': 1.0742, 'learning_rate': 0.00022727272727272727, 'epoch': 1.55}\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 17:06:43,897] [WARNING] [stage3.py:1939:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding torch.cuda.empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 17:06:43,898] [INFO] [logging.py:68:log_dist] [Rank 0] step=18, skipped=0, lr=[0.00018181818181818183], mom=[[0.9, 0.999]]\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 17:06:43,898] [INFO] [timer.py:196:stop] epoch=0/micro_step=18/global_step=18, RunningAvgSamplesPerSec=2.860517956458437, CurrSamplesPerSec=2.838397455372307, MemAllocated=0.85GB, MaxMemAllocated=12.87GB\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) {'loss': 0.9956, 'learning_rate': 0.00018181818181818183, 'epoch': 1.64}\n",
      "(RayTrainWorker pid=66667, ip=10.0.15.54) {'loss': 0.9956, 'learning_rate': 0.00018181818181818183, 'epoch': 1.64}\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 17:07:17,614] [WARNING] [stage3.py:1939:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding torch.cuda.empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 17:07:17,615] [INFO] [logging.py:68:log_dist] [Rank 0] step=19, skipped=0, lr=[0.00013636363636363637], mom=[[0.9, 0.999]]\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 17:07:17,615] [INFO] [timer.py:196:stop] epoch=0/micro_step=19/global_step=19, RunningAvgSamplesPerSec=2.8597867171090887, CurrSamplesPerSec=2.8481375248315275, MemAllocated=0.85GB, MaxMemAllocated=12.87GB\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) {'loss': 0.9768, 'learning_rate': 0.00013636363636363637, 'epoch': 1.73}\n",
      "(RayTrainWorker pid=66667, ip=10.0.15.54) {'loss': 0.9768, 'learning_rate': 0.00013636363636363637, 'epoch': 1.73}\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 17:07:51,504] [WARNING] [stage3.py:1939:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding torch.cuda.empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 17:07:51,505] [INFO] [logging.py:68:log_dist] [Rank 0] step=20, skipped=0, lr=[9.090909090909092e-05], mom=[[0.9, 0.999]]\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 17:07:51,506] [INFO] [timer.py:196:stop] epoch=0/micro_step=20/global_step=20, RunningAvgSamplesPerSec=2.8583167095894018, CurrSamplesPerSec=2.833555799541949, MemAllocated=0.85GB, MaxMemAllocated=12.87GB\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) {'loss': 0.9619, 'learning_rate': 9.090909090909092e-05, 'epoch': 1.82}\n",
      "(RayTrainWorker pid=66667, ip=10.0.15.54) {'loss': 0.9619, 'learning_rate': 9.090909090909092e-05, 'epoch': 1.82}\n",
      "(RayTrainWorker pid=66667, ip=10.0.15.54) {'loss': 0.9622, 'learning_rate': 4.545454545454546e-05, 'epoch': 1.91}\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 17:08:25,527] [WARNING] [stage3.py:1939:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding torch.cuda.empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 17:08:25,528] [INFO] [logging.py:68:log_dist] [Rank 0] step=21, skipped=0, lr=[4.545454545454546e-05], mom=[[0.9, 0.999]]\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 17:08:25,529] [INFO] [timer.py:196:stop] epoch=0/micro_step=21/global_step=21, RunningAvgSamplesPerSec=2.8564069684185123, CurrSamplesPerSec=2.822462821557566, MemAllocated=0.85GB, MaxMemAllocated=12.87GB\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) {'loss': 0.9622, 'learning_rate': 4.545454545454546e-05, 'epoch': 1.91}\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 17:08:58,832] [WARNING] [stage3.py:1939:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding torch.cuda.empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 17:08:58,833] [INFO] [logging.py:68:log_dist] [Rank 0] step=22, skipped=0, lr=[0.0], mom=[[0.9, 0.999]]\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) [2023-02-09 17:08:58,833] [INFO] [timer.py:196:stop] epoch=0/micro_step=22/global_step=22, RunningAvgSamplesPerSec=2.8577366387262413, CurrSamplesPerSec=2.883237681107283, MemAllocated=0.85GB, MaxMemAllocated=12.87GB\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) {'loss': 0.9338, 'learning_rate': 0.0, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=75470, ip=10.0.55.91) Saving model checkpoint to gpt-j-6B/checkpoint-22\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) Configuration saved in gpt-j-6B/checkpoint-22/config.json\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) Configuration saved in gpt-j-6B/checkpoint-22/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=66667, ip=10.0.15.54) {'loss': 0.9338, 'learning_rate': 0.0, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=75470, ip=10.0.55.91) Model weights saved in gpt-j-6B/checkpoint-22/pytorch_model.bin\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) tokenizer config file saved in gpt-j-6B/checkpoint-22/tokenizer_config.json\n",
      "(RayTrainWorker pid=75470, ip=10.0.55.91) Special tokens file saved in gpt-j-6B/checkpoint-22/special_tokens_map.json\n",
      "2023-02-09 17:09:00,208\tERROR trial_runner.py:1062 -- Trial HuggingFaceTrainerPatched_82a20_00000: Error processing event.\n",
      "ray.exceptions.RayTaskError(PicklingError): \u001b[36mray::_Inner.train()\u001b[39m (pid=74513, ip=10.0.55.91, repr=HuggingFaceTrainerPatched)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/tune/trainable/trainable.py\", line 368, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", line 54, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "ray.exceptions.RayTaskError(PicklingError): \u001b[36mray::RayTrainWorker._RayTrainWorker__execute()\u001b[39m (pid=75470, ip=10.0.55.91, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x7f54b0efe700>)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/worker_group.py\", line 31, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", line 129, in discard_return_wrapper\n",
      "    train_func(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/huggingface/huggingface_trainer.py\", line 417, in _huggingface_train_loop_per_worker\n",
      "    trainer.train()\n",
      "  File \"/mnt/cluster_storage/pypi/lib/python3.8/site-packages/transformers/trainer.py\", line 1543, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/mnt/cluster_storage/pypi/lib/python3.8/site-packages/transformers/trainer.py\", line 1883, in _inner_training_loop\n",
      "    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n",
      "  File \"/mnt/cluster_storage/pypi/lib/python3.8/site-packages/transformers/trainer.py\", line 2135, in _maybe_log_save_evaluate\n",
      "    self._save_checkpoint(model, trial, metrics=metrics)\n",
      "  File \"/mnt/cluster_storage/pypi/lib/python3.8/site-packages/transformers/trainer.py\", line 2192, in _save_checkpoint\n",
      "    self.save_model(output_dir, _internal_call=True)\n",
      "  File \"/mnt/cluster_storage/pypi/lib/python3.8/site-packages/transformers/trainer.py\", line 2646, in save_model\n",
      "    self._save(output_dir)\n",
      "  File \"/mnt/cluster_storage/pypi/lib/python3.8/site-packages/transformers/trainer.py\", line 2728, in _save\n",
      "    torch.save(self.args, os.path.join(output_dir, TRAINING_ARGS_NAME))\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/torch/serialization.py\", line 423, in save\n",
      "    _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/torch/serialization.py\", line 635, in _save\n",
      "    pickler.dump(obj)\n",
      "_pickle.PicklingError: Can't pickle <class '__main__.TrainingArgumentsPatched'>: attribute lookup TrainingArgumentsPatched on __main__ failed\n",
      "2023-02-09 17:09:00,272\tERROR tune.py:794 -- Trials did not complete: [HuggingFaceTrainerPatched_82a20_00000]\n",
      "2023-02-09 17:09:00,273\tINFO tune.py:798 -- Total run time: 855.15 seconds (855.10 seconds for the tuning loop).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\"> </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> </span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>1 results = trainer.fit()                                                                      <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">base_trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">368</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">fit</span>             <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">365       </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                               <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">366          </span>result = result_grid[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>]                                                        <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">367          </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> result.error:                                                               <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>368 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> result.error                                                         <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">369       </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">except</span> TuneError <span style=\"color: #0000ff; text-decoration-color: #0000ff\">as</span> e:                                                             <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">370          </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> TrainingFailedError <span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">e</span>                                               <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">371       </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> result                                                                      <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">RayTaskError(PicklingError): </span>\u001b<span style=\"font-weight: bold\">[</span>36mray::<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">_Inner.train</span><span style=\"font-weight: bold\">()</span>\u001b<span style=\"font-weight: bold\">[</span>39m <span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">pid</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">74513</span>, <span style=\"color: #808000; text-decoration-color: #808000\">ip</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">10</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">.0.55.91</span>, <span style=\"color: #808000; text-decoration-color: #808000\">repr</span>=<span style=\"color: #800080; text-decoration-color: #800080\">HuggingFaceTrainerPatched</span><span style=\"font-weight: bold\">)</span>\n",
       "  File <span style=\"color: #008000; text-decoration-color: #008000\">\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/tune/trainable/trainable.py\"</span>, line <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">368</span>, in train\n",
       "    raise skipped from <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">exception_cause</span><span style=\"font-weight: bold\">(</span>skipped<span style=\"font-weight: bold\">)</span>\n",
       "  File <span style=\"color: #008000; text-decoration-color: #008000\">\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\"</span>, line <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">54</span>, in \n",
       "check_for_failure\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ray.get</span><span style=\"font-weight: bold\">(</span>object_ref<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ray.exceptions.RayTaskError</span><span style=\"font-weight: bold\">(</span>PicklingError<span style=\"font-weight: bold\">)</span>: \u001b<span style=\"font-weight: bold\">[</span>36mray::<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RayTrainWorker._RayTrainWorker__execute</span><span style=\"font-weight: bold\">()</span>\u001b<span style=\"font-weight: bold\">[</span>39m <span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">pid</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">75470</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">ip</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">10</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">.0.55.91</span>, <span style=\"color: #808000; text-decoration-color: #808000\">repr</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">ray.train._internal.worker_group.RayTrainWorker</span><span style=\"color: #000000; text-decoration-color: #000000\"> object at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x7f54b0efe700</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">  File </span><span style=\"color: #008000; text-decoration-color: #008000\">\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/worker_group.py\"</span><span style=\"color: #000000; text-decoration-color: #000000\">, line </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">31</span><span style=\"color: #000000; text-decoration-color: #000000\">, in __execute</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    raise skipped from </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">exception_cause</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">skipped</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">  File </span><span style=\"color: #008000; text-decoration-color: #008000\">\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\"</span><span style=\"color: #000000; text-decoration-color: #000000\">, line </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">129</span><span style=\"color: #000000; text-decoration-color: #000000\">, in </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">discard_return_wrapper</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">train_func</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">*args, **kwargs</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">  File </span><span style=\"color: #008000; text-decoration-color: #008000\">\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/huggingface/huggingface_trainer.py\"</span><span style=\"color: #000000; text-decoration-color: #000000\">, line </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">417</span><span style=\"color: #000000; text-decoration-color: #000000\">, in</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">_huggingface_train_loop_per_worker</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">trainer.train</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">()</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">  File </span><span style=\"color: #008000; text-decoration-color: #008000\">\"/mnt/cluster_storage/pypi/lib/python3.8/site-packages/transformers/trainer.py\"</span><span style=\"color: #000000; text-decoration-color: #000000\">, line </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1543</span><span style=\"color: #000000; text-decoration-color: #000000\">, in train</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    return </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">inner_training_loop</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">  File </span><span style=\"color: #008000; text-decoration-color: #008000\">\"/mnt/cluster_storage/pypi/lib/python3.8/site-packages/transformers/trainer.py\"</span><span style=\"color: #000000; text-decoration-color: #000000\">, line </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1883</span><span style=\"color: #000000; text-decoration-color: #000000\">, in </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">_inner_training_loop</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">self._maybe_log_save_evaluate</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">tr_loss, model, trial, epoch, ignore_keys_for_eval</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">  File </span><span style=\"color: #008000; text-decoration-color: #008000\">\"/mnt/cluster_storage/pypi/lib/python3.8/site-packages/transformers/trainer.py\"</span><span style=\"color: #000000; text-decoration-color: #000000\">, line </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2135</span><span style=\"color: #000000; text-decoration-color: #000000\">, in </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">_maybe_log_save_evaluate</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">self._save_checkpoint</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">model, trial, </span><span style=\"color: #808000; text-decoration-color: #808000\">metrics</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #800080; text-decoration-color: #800080\">metrics</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">  File </span><span style=\"color: #008000; text-decoration-color: #008000\">\"/mnt/cluster_storage/pypi/lib/python3.8/site-packages/transformers/trainer.py\"</span><span style=\"color: #000000; text-decoration-color: #000000\">, line </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2192</span><span style=\"color: #000000; text-decoration-color: #000000\">, in </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">_save_checkpoint</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">self.save_model</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">output_dir, </span><span style=\"color: #808000; text-decoration-color: #808000\">_internal_call</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">  File </span><span style=\"color: #008000; text-decoration-color: #008000\">\"/mnt/cluster_storage/pypi/lib/python3.8/site-packages/transformers/trainer.py\"</span><span style=\"color: #000000; text-decoration-color: #000000\">, line </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2646</span><span style=\"color: #000000; text-decoration-color: #000000\">, in save_model</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">self._save</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">output_dir</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">  File </span><span style=\"color: #008000; text-decoration-color: #008000\">\"/mnt/cluster_storage/pypi/lib/python3.8/site-packages/transformers/trainer.py\"</span><span style=\"color: #000000; text-decoration-color: #000000\">, line </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2728</span><span style=\"color: #000000; text-decoration-color: #000000\">, in _save</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.save</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">self.args, </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">os.path.join</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">output_dir, TRAINING_ARGS_NAME</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">))</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">  File </span><span style=\"color: #008000; text-decoration-color: #008000\">\"/home/ray/anaconda3/lib/python3.8/site-packages/torch/serialization.py\"</span><span style=\"color: #000000; text-decoration-color: #000000\">, line </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">423</span><span style=\"color: #000000; text-decoration-color: #000000\">, in save</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">_save</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">obj, opened_zipfile, pickle_module, pickle_protocol</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">  File </span><span style=\"color: #008000; text-decoration-color: #008000\">\"/home/ray/anaconda3/lib/python3.8/site-packages/torch/serialization.py\"</span><span style=\"color: #000000; text-decoration-color: #000000\">, line </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">635</span><span style=\"color: #000000; text-decoration-color: #000000\">, in _save</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">pickler.dump</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">obj</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">_pickle.PicklingError: Can't pickle &lt;class </span><span style=\"color: #008000; text-decoration-color: #008000\">'__main__.TrainingArgumentsPatched'</span><span style=\"font-weight: bold\">&gt;</span>: attribute lookup \n",
       "TrainingArgumentsPatched on __main__ failed\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m\u001b[0m\u001b[31m\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m\u001b[0m\u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m in \u001b[92m<module>\u001b[0m                                                                                      \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m1 results = trainer.fit()                                                                      \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m2 \u001b[0m                                                                                             \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[2;33m/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/\u001b[0m\u001b[1;33mbase_trainer.py\u001b[0m:\u001b[94m368\u001b[0m in \u001b[92mfit\u001b[0m             \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m365 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mtry\u001b[0m:                                                                               \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m366 \u001b[0m\u001b[2m         \u001b[0mresult = result_grid[\u001b[94m0\u001b[0m]                                                        \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m367 \u001b[0m\u001b[2m         \u001b[0m\u001b[94mif\u001b[0m result.error:                                                               \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m368 \u001b[2m            \u001b[0m\u001b[94mraise\u001b[0m result.error                                                         \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m369 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mexcept\u001b[0m TuneError \u001b[94mas\u001b[0m e:                                                             \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m370 \u001b[0m\u001b[2m         \u001b[0m\u001b[94mraise\u001b[0m TrainingFailedError \u001b[94mfrom\u001b[0m \u001b[4;96me\u001b[0m                                               \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m371 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mreturn\u001b[0m result                                                                      \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m\n",
       "\u001b[1;91mRayTaskError(PicklingError): \u001b[0m\u001b\u001b[1m[\u001b[0m36mray::\u001b[1;35m_Inner.train\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\u001b\u001b[1m[\u001b[0m39m \u001b[1m(\u001b[0m\u001b[33mpid\u001b[0m=\u001b[1;36m74513\u001b[0m, \u001b[33mip\u001b[0m=\u001b[1;92m10\u001b[0m\u001b[1;92m.0.55.91\u001b[0m, \u001b[33mrepr\u001b[0m=\u001b[35mHuggingFaceTrainerPatched\u001b[0m\u001b[1m)\u001b[0m\n",
       "  File \u001b[32m\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/tune/trainable/trainable.py\"\u001b[0m, line \u001b[1;36m368\u001b[0m, in train\n",
       "    raise skipped from \u001b[1;35mexception_cause\u001b[0m\u001b[1m(\u001b[0mskipped\u001b[1m)\u001b[0m\n",
       "  File \u001b[32m\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\"\u001b[0m, line \u001b[1;36m54\u001b[0m, in \n",
       "check_for_failure\n",
       "    \u001b[1;35mray.get\u001b[0m\u001b[1m(\u001b[0mobject_ref\u001b[1m)\u001b[0m\n",
       "\u001b[1;35mray.exceptions.RayTaskError\u001b[0m\u001b[1m(\u001b[0mPicklingError\u001b[1m)\u001b[0m: \u001b\u001b[1m[\u001b[0m36mray::\u001b[1;35mRayTrainWorker._RayTrainWorker__execute\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\u001b\u001b[1m[\u001b[0m39m \u001b[1m(\u001b[0m\u001b[33mpid\u001b[0m=\u001b[1;36m75470\u001b[0m, \n",
       "\u001b[33mip\u001b[0m=\u001b[1;92m10\u001b[0m\u001b[1;92m.0.55.91\u001b[0m, \u001b[33mrepr\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mray.train._internal.worker_group.RayTrainWorker\u001b[0m\u001b[39m object at \u001b[0m\u001b[1;36m0x7f54b0efe700\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\n",
       "\u001b[39m  File \u001b[0m\u001b[32m\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/worker_group.py\"\u001b[0m\u001b[39m, line \u001b[0m\u001b[1;36m31\u001b[0m\u001b[39m, in __execute\u001b[0m\n",
       "\u001b[39m    raise skipped from \u001b[0m\u001b[1;35mexception_cause\u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mskipped\u001b[0m\u001b[1;39m)\u001b[0m\n",
       "\u001b[39m  File \u001b[0m\u001b[32m\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\"\u001b[0m\u001b[39m, line \u001b[0m\u001b[1;36m129\u001b[0m\u001b[39m, in \u001b[0m\n",
       "\u001b[39mdiscard_return_wrapper\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[1;35mtrain_func\u001b[0m\u001b[1;39m(\u001b[0m\u001b[39m*args, **kwargs\u001b[0m\u001b[1;39m)\u001b[0m\n",
       "\u001b[39m  File \u001b[0m\u001b[32m\"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/huggingface/huggingface_trainer.py\"\u001b[0m\u001b[39m, line \u001b[0m\u001b[1;36m417\u001b[0m\u001b[39m, in\u001b[0m\n",
       "\u001b[39m_huggingface_train_loop_per_worker\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[1;35mtrainer.train\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m)\u001b[0m\n",
       "\u001b[39m  File \u001b[0m\u001b[32m\"/mnt/cluster_storage/pypi/lib/python3.8/site-packages/transformers/trainer.py\"\u001b[0m\u001b[39m, line \u001b[0m\u001b[1;36m1543\u001b[0m\u001b[39m, in train\u001b[0m\n",
       "\u001b[39m    return \u001b[0m\u001b[1;35minner_training_loop\u001b[0m\u001b[1;39m(\u001b[0m\n",
       "\u001b[39m  File \u001b[0m\u001b[32m\"/mnt/cluster_storage/pypi/lib/python3.8/site-packages/transformers/trainer.py\"\u001b[0m\u001b[39m, line \u001b[0m\u001b[1;36m1883\u001b[0m\u001b[39m, in \u001b[0m\n",
       "\u001b[39m_inner_training_loop\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[1;35mself._maybe_log_save_evaluate\u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mtr_loss, model, trial, epoch, ignore_keys_for_eval\u001b[0m\u001b[1;39m)\u001b[0m\n",
       "\u001b[39m  File \u001b[0m\u001b[32m\"/mnt/cluster_storage/pypi/lib/python3.8/site-packages/transformers/trainer.py\"\u001b[0m\u001b[39m, line \u001b[0m\u001b[1;36m2135\u001b[0m\u001b[39m, in \u001b[0m\n",
       "\u001b[39m_maybe_log_save_evaluate\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[1;35mself._save_checkpoint\u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mmodel, trial, \u001b[0m\u001b[33mmetrics\u001b[0m\u001b[39m=\u001b[0m\u001b[35mmetrics\u001b[0m\u001b[1;39m)\u001b[0m\n",
       "\u001b[39m  File \u001b[0m\u001b[32m\"/mnt/cluster_storage/pypi/lib/python3.8/site-packages/transformers/trainer.py\"\u001b[0m\u001b[39m, line \u001b[0m\u001b[1;36m2192\u001b[0m\u001b[39m, in \u001b[0m\n",
       "\u001b[39m_save_checkpoint\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[1;35mself.save_model\u001b[0m\u001b[1;39m(\u001b[0m\u001b[39moutput_dir, \u001b[0m\u001b[33m_internal_call\u001b[0m\u001b[39m=\u001b[0m\u001b[3;92mTrue\u001b[0m\u001b[1;39m)\u001b[0m\n",
       "\u001b[39m  File \u001b[0m\u001b[32m\"/mnt/cluster_storage/pypi/lib/python3.8/site-packages/transformers/trainer.py\"\u001b[0m\u001b[39m, line \u001b[0m\u001b[1;36m2646\u001b[0m\u001b[39m, in save_model\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[1;35mself._save\u001b[0m\u001b[1;39m(\u001b[0m\u001b[39moutput_dir\u001b[0m\u001b[1;39m)\u001b[0m\n",
       "\u001b[39m  File \u001b[0m\u001b[32m\"/mnt/cluster_storage/pypi/lib/python3.8/site-packages/transformers/trainer.py\"\u001b[0m\u001b[39m, line \u001b[0m\u001b[1;36m2728\u001b[0m\u001b[39m, in _save\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[1;35mtorch.save\u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mself.args, \u001b[0m\u001b[1;35mos.path.join\u001b[0m\u001b[1;39m(\u001b[0m\u001b[39moutput_dir, TRAINING_ARGS_NAME\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;39m)\u001b[0m\n",
       "\u001b[39m  File \u001b[0m\u001b[32m\"/home/ray/anaconda3/lib/python3.8/site-packages/torch/serialization.py\"\u001b[0m\u001b[39m, line \u001b[0m\u001b[1;36m423\u001b[0m\u001b[39m, in save\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[1;35m_save\u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mobj, opened_zipfile, pickle_module, pickle_protocol\u001b[0m\u001b[1;39m)\u001b[0m\n",
       "\u001b[39m  File \u001b[0m\u001b[32m\"/home/ray/anaconda3/lib/python3.8/site-packages/torch/serialization.py\"\u001b[0m\u001b[39m, line \u001b[0m\u001b[1;36m635\u001b[0m\u001b[39m, in _save\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[1;35mpickler.dump\u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mobj\u001b[0m\u001b[1;39m)\u001b[0m\n",
       "\u001b[39m_pickle.PicklingError: Can't pickle <class \u001b[0m\u001b[32m'__main__.TrainingArgumentsPatched'\u001b[0m\u001b[1m>\u001b[0m: attribute lookup \n",
       "TrainingArgumentsPatched on __main__ failed\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.air import Checkpoint\n",
    "checkpoint = Checkpoint.from_directory(\"/mnt/cluster_storage/HuggingFaceTrainer_2023-02-08_14-55-52/HuggingFaceTrainer_bca80_00000_0_2023-02-08_14-55-53/rank_0/gpt-j-6B/checkpoint-394/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train.huggingface import HuggingFaceCheckpoint\n",
    "from ray.air._internal.checkpointing import (\n",
    "    load_preprocessor_from_dir,\n",
    "    save_preprocessor_to_dir,\n",
    ")\n",
    "\n",
    "class HuggingFaceCheckpointPatched(HuggingFaceCheckpoint):\n",
    "    def get_preprocessor(self):\n",
    "        \"\"\"Return the saved preprocessor, if one exists.\"\"\"\n",
    "\n",
    "        # The preprocessor will either be stored in an in-memory dict or\n",
    "        # written to storage. In either case, it will use the PREPROCESSOR_KEY key.\n",
    "\n",
    "        with self.as_directory() as checkpoint_path:\n",
    "            preprocessor = load_preprocessor_from_dir(checkpoint_path)\n",
    "\n",
    "        return preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train.huggingface import HuggingFacePredictor\n",
    "from transformers import set_seed\n",
    "\n",
    "@ray.remote(num_gpus=1)\n",
    "def predict(uri, seed=None):\n",
    "    if seed is None:\n",
    "        rng = np.random.default_rng(seed=None)\n",
    "        seed = rng.integers(0, 2**16)\n",
    "    print(f\"seed: {seed}\")\n",
    "    set_seed(seed)\n",
    "    checkpoint = HuggingFaceCheckpointPatched.from_uri(uri)\n",
    "    print(\"creating predictor\")\n",
    "    predictor = HuggingFacePredictor.from_checkpoint(checkpoint, task=\"text-generation\", device=0, torch_dtype=torch.bfloat16)\n",
    "    # No need to use AIR preprocessor, and it looks like the one I coded has\n",
    "    # issues with being loaded, so we just get rid of it\n",
    "    predictor._preprocessor = None\n",
    "    print(\"predicting\")\n",
    "    return predictor.predict(\n",
    "        pd.DataFrame([[\"Romeo:\"]]),\n",
    "        do_sample=True, \n",
    "        max_new_tokens=256, \n",
    "        top_k=50, \n",
    "        top_p=0.95, \n",
    "        num_return_sequences=3\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_tasks = [predict.remote(checkpoint.uri) for i in range(8)]\n",
    "predictions = ray.get(prediction_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
